# -*- coding: utf-8 -*-
"""echo 4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fX-lTpWeirQPRFe9xncQOpGM1AEvDgMk
"""

# Advanced Tamil EchoVerse - Multi-Character Natural Voice Generation
# Google Colab Notebook with Premium TTS Models (Attempting advanced install again)

# ============================================================================
# CELL 1: Install Advanced Dependencies
# ============================================================================

!pip install -q gradio transformers torch torchaudio
!pip install -q pdfplumber PyMuPDF
!pip install -q pydub nltk spacy
!pip install -q datasets accelerate
!pip install -q TTS  # Install the latest compatible version of Coqui TTS (supports XTTS v2)
!pip install -q speechbrain  # Advanced speech processing
!pip install -q resemblyzer  # Voice cloning and speaker embedding
!pip install -q librosa soundfile scipy
!pip install -q phonemizer  # Better pronunciation
!apt-get -qq install ffmpeg espeak-ng
!pip install -q bark  # Bark TTS for natural voices
!pip install -q tortoise-tts  # Ultra-realistic TTS
!pip install -q real-esrgan  # Audio enhancement
!pip install -q noisereduce  # Audio denoising

# Tamil-specific advanced libraries
!pip install -q indic-transliteration
!pip install -q polyglot
!pip install -q indic-nlp-library # Alternative to tamil-nltk
!pip install -q ai4bharat-transliteration
!pip install -q gtts # Keep gtts as a fallback


# Download language models
!python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
print("‚úÖ Attempting to install advanced dependencies...")
print("‚ö†Ô∏è Note: Some advanced libraries may fail to install due to environment compatibility.")


# ============================================================================
# CELL 2: Import Libraries and Setup
# ============================================================================

import gradio as gr
import torch
import torchaudio
import numpy as np
import pandas as pd
from transformers import pipeline
import pdfplumber
import fitz  # PyMuPDF
import re
import os
import tempfile
import json
from datetime import datetime
from pydub import AudioSegment
from pydub.effects import normalize, compress_dynamic_range, low_pass_filter, high_pass_filter
from pydub.generators import Sine, Sawtooth, Triangle, Square
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import librosa
import soundfile as sf
from scipy import signal
import noisereduce as nr
import warnings
warnings.filterwarnings('ignore')

# Advanced TTS imports (Try importing the advanced libraries again)
try:
    from TTS.api import TTS
    from TTS.tts.configs.xtts_config import XttsConfig
    from TTS.tts.models.xtts import Xtts
    ADVANCED_TTS_AVAILABLE = True
    print("‚úÖ Advanced TTS (Coqui TTS) imported successfully!")
except ImportError:
    print("‚ö†Ô∏è Advanced TTS (Coqui TTS) not available.")
    ADVANCED_TTS_AVAILABLE = False

try:
    from resemblyzer import VoiceEncoder, preprocess_wav
    VOICE_CLONING_AVAILABLE = True
    print("‚úÖ Voice cloning (Resembyzer) imported successfully!")
except ImportError:
    print("‚ö†Ô∏è Voice cloning (Resembyzer) not available.")
    VOICE_CLONING_AVAILABLE = False
    # Dummy function if import fails
    def preprocess_wav(fpath):
        print("‚ö†Ô∏è preprocess_wav not available. Voice cloning disabled.")
        return None
    class VoiceEncoder:
        def embed_utterance(self, wav):
            print("‚ö†Ô∏è embed_utterance not available. Voice cloning disabled.")
            return None


try:
    from bark import SAMPLE_RATE, generate_audio, preload_models
    BARK_AVAILABLE = True
    print("‚úÖ Bark TTS imported successfully!")
except ImportError:
    print("‚ö†Ô∏è Bark TTS not available.")
    BARK_AVAILABLE = False

# Import Google TTS (for fallback)
try:
    from gtts import gTTS
    GTTS_AVAILABLE = True
    print("‚úÖ Google TTS (gTTS) imported successfully!")
except ImportError:
    print("‚ùå Google TTS (gTTS) is not available. Fallback not possible.")
    GTTS_AVAILABLE = False


print("‚úÖ Essential libraries imported successfully!")


# ============================================================================
# CELL 3: Initialize Advanced Tamil TTS Models
# ============================================================================

class AdvancedTamilTTS:
    def __init__(self):
        self.models = {}
        self.voice_encoder = None
        self.character_voices = {}
        self.load_models()

    def load_models(self):
        """Load all available TTS models with priority ranking"""
        print("üöÄ Loading Advanced Tamil TTS Models...")

        # Model 1: XTTS v2 (Best quality, supports voice cloning)
        if ADVANCED_TTS_AVAILABLE:
            try:
                print("üìö Loading XTTS v2 (Multilingual)...")
                self.models['xtts_v2'] = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
                print("‚úÖ XTTS v2 loaded successfully!")
            except Exception as e:
                print(f"‚ö†Ô∏è XTTS v2 failed: {e}")
                self.models['xtts_v2'] = None
        else:
             self.models['xtts_v2'] = None


        # Model 2: Bark TTS (Most natural sounding)
        if BARK_AVAILABLE:
            try:
                print("üé≠ Loading Bark TTS (Ultra-natural)...")
                preload_models() # This might download models
                self.models['bark'] = True
                print("‚úÖ Bark TTS loaded successfully!")
            except Exception as e:
                print(f"‚ö†Ô∏è Bark TTS failed: {e}")
                self.models['bark'] = None
        else:
             self.models['bark'] = None


        # Model 3: YourTTS (Multi-speaker)
        if ADVANCED_TTS_AVAILABLE: # YourTTS is also part of Coqui TTS
            try:
                print("üé™ Loading YourTTS (Multi-speaker)...")
                self.models['yourtts'] = TTS("tts_models/multilingual/multi-dataset/your_tts")
                print("‚úÖ YourTTS loaded successfully!")
            except Exception as e:
                print(f"‚ö†Ô∏è YourTTS failed: {e}")
                self.models['yourtts'] = None
        else:
             self.models['yourtts'] = None


        # Model 4: Tamil specific model (from Coqui TTS)
        if ADVANCED_TTS_AVAILABLE: # Tamil model is also part of Coqui TTS
            try:
                print("üáÆüá≥ Loading Tamil Tacotron2...")
                self.models['tamil_tacotron'] = TTS("tts_models/ta/mai/tacotron2-DDC")
                print("‚úÖ Tamil Tacotron2 loaded successfully!")
            except Exception as e:
                print(f"‚ö†Ô∏è Tamil Tacotron2 failed: {e}")
                self.models['tamil_tacotron'] = None
        else:
             self.models['tamil_tacotron'] = None


        # Voice encoder for cloning (Requires Resembyzer)
        if VOICE_CLONING_AVAILABLE:
            try:
                print("üé§ Loading Voice Encoder for cloning...")
                self.voice_encoder = VoiceEncoder()
                print("‚úÖ Voice encoder loaded successfully!")
            except Exception as e:
                print(f"‚ö†Ô∏è Voice encoder failed: {e}")
                self.voice_encoder = None
        else:
             self.voice_encoder = None


        # Fallback: Google TTS
        if GTTS_AVAILABLE:
             self.models['gtts'] = True
             print("‚úÖ Google TTS Tamil confirmed as fallback!")
        else:
             self.models['gtts'] = None
             print("‚ùå Google TTS fallback not available.")


        # Load predefined character voices
        self.load_character_voices()

        print("üéâ Advanced TTS initialization complete!")

    def load_character_voices(self):
        """Load predefined character voice profiles"""
        # Restoring original character profiles
        self.character_voices = {
            "narrator": {
                "name": "‡Æï‡Æ§‡Øà ‡Æö‡Øä‡Æ≤‡Øç‡Æ™‡Æµ‡Æ∞‡Øç",
                "description": "Professional narrator voice",
                "pitch_shift": 0,
                "speed_factor": 1.0,
                "tone": "neutral",
                "gender": "neutral"
            },
            "male_young": {
                "name": "‡Æá‡Æ≥‡ÆÆ‡Øç ‡ÆÜ‡Æ£‡Øç",
                "description": "Young male character",
                "pitch_shift": -2,
                "speed_factor": 1.1,
                "tone": "energetic",
                "gender": "male"
            },
            "female_young": {
                "name": "‡Æá‡Æ≥‡ÆÆ‡Øç ‡Æ™‡ØÜ‡Æ£‡Øç",
                "description": "Young female character",
                "pitch_shift": 3,
                "speed_factor": 1.05,
                "tone": "gentle",
                "gender": "female"
            },
            "male_old": {
                "name": "‡Æµ‡ÆØ‡Æ§‡Ææ‡Æ© ‡ÆÜ‡Æ£‡Øç",
                "description": "Elderly male character",
                "pitch_shift": -4,
                "speed_factor": 0.9,
                "tone": "wise",
                "gender": "male"
            },
            "female_old": {
                "name": "‡Æµ‡ÆØ‡Æ§‡Ææ‡Æ© ‡Æ™‡ØÜ‡Æ£‡Øç",
                "description": "Elderly female character",
                "pitch_shift": 1,
                "speed_factor": 0.85,
                "tone": "warm",
                "gender": "female"
            },
            "child": {
                "name": "‡Æï‡ØÅ‡Æ¥‡Æ®‡Øç‡Æ§‡Øà",
                "description": "Child character",
                "pitch_shift": 6,
                "speed_factor": 1.2,
                "tone": "playful",
                "gender": "child"
            },
            "villain": {
                "name": "‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Æ©‡Øç",
                "description": "Antagonist character",
                "pitch_shift": -3,
                "speed_factor": 0.95,
                "tone": "menacing",
                "gender": "male"
            },
            "hero": {
                "name": "‡Æ®‡Ææ‡ÆØ‡Æï‡Æ©‡Øç",
                "description": "Hero character",
                "pitch_shift": -1,
                "speed_factor": 1.0,
                "tone": "confident",
                "gender": "male"
            }
        }

# Initialize the advanced TTS system
tts_system = AdvancedTamilTTS()


# ============================================================================
# CELL 4: Enhanced Audio Processing Functions (Keep these)
# ============================================================================

def enhance_audio_quality(audio_segment):
    """Apply advanced audio enhancement"""
    try:
        print("üéöÔ∏è Enhancing audio quality...")

        # Convert to numpy for processing (assuming input is AudioSegment)
        audio_data = np.array(audio_segment.get_array_of_samples())

        # Noise reduction (requires noisereduce)
        try:
            import noisereduce as nr_check # Check if noisereduce is available
            audio_data = nr.reduce_noise(y=audio_data, sr=audio_segment.frame_rate)
            print("‚úÖ Noise reduction applied.")
        except ImportError:
             print("‚ö†Ô∏è noisereduce not available for enhancement.")


        # Convert back to AudioSegment
        enhanced_audio = AudioSegment(
            audio_data.tobytes(),
            frame_rate=audio_segment.frame_rate,
            sample_width=audio_segment.sample_width,
            channels=audio_segment.channels
        )

        # Apply audio effects (normalize, filters)
        enhanced_audio = normalize(enhanced_audio)
        enhanced_audio = high_pass_filter(enhanced_audio, 80)  # Remove low rumble
        enhanced_audio = low_pass_filter(enhanced_audio, 8000)  # Remove harsh highs


        print("‚úÖ Audio quality enhanced!")
        return enhanced_audio

    except Exception as e:
        print(f"‚ö†Ô∏è Audio enhancement failed: {e}")
        return audio_segment

def apply_character_voice_effects(audio_segment, character_profile):
    """Apply character-specific voice effects"""
    try:
        print(f"üé≠ Applying {character_profile['name']} voice effects...")

        # Speed adjustment
        if character_profile['speed_factor'] != 1.0:
            audio_segment = audio_segment.speedup(playback_speed=character_profile['speed_factor'])


        # Pitch shifting (basic implementation)
        pitch_shift = character_profile['pitch_shift']
        if pitch_shift != 0:
            # Convert to raw audio data
            audio_data = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)

            # Simple pitch shifting using resampling
            if pitch_shift > 0:
                # Higher pitch
                factor = 1 + (pitch_shift * 0.1)
                new_rate = int(audio_segment.frame_rate * factor)
                audio_segment = audio_segment._spawn(audio_data, overrides={
                    "frame_rate": new_rate
                }).set_frame_rate(audio_segment.frame_rate)
            elif pitch_shift < 0:
                # Lower pitch
                factor = 1 + (pitch_shift * 0.1)
                new_rate = int(audio_segment.frame_rate * factor)
                audio_segment = audio_segment._spawn(audio_data, overrides={
                    "frame_rate": new_rate
                }).set_frame_rate(audio_segment.frame_rate)

        # Tone-specific effects
        tone = character_profile['tone']
        if tone == "energetic":
            audio_segment = audio_segment + 2  # Slightly louder
        elif tone == "gentle":
            audio_segment = audio_segment - 1  # Slightly softer
        elif tone == "wise":
            audio_segment = compress_dynamic_range(audio_segment)
        elif tone == "menacing":
            audio_segment = audio_segment + 3  # Louder and more aggressive
            audio_segment = low_pass_filter(audio_segment, 6000)
        elif tone == "playful":
            audio_segment = audio_segment + 1
        elif tone == "confident":
            audio_segment = normalize(audio_segment)

        print(f"‚úÖ {character_profile['name']} effects applied!")
        return audio_segment

    except Exception as e:
        print(f"‚ö†Ô∏è Character effects failed: {e}")
        return audio_segment

def clone_voice_from_sample(reference_audio_path):
    """Extract voice characteristics from reference audio (Requires Resembyzer)"""
    try:
        if not VOICE_CLONING_AVAILABLE or not tts_system.voice_encoder:
            print("‚ö†Ô∏è Voice cloning is not available.")
            return None

        print("üé§ Analyzing reference voice...")

        # Load and preprocess audio
        wav = preprocess_wav(reference_audio_path)

        # Extract voice embedding
        embedding = tts_system.voice_encoder.embed_utterance(wav)

        print("‚úÖ Voice embedding extracted!")
        return embedding

    except Exception as e:
        print(f"‚ö†Ô∏è Voice cloning failed: {e}")
        return None

# ============================================================================
# CELL 5: Advanced Text Processing for Tamil (Restored)
# ============================================================================

def advanced_tamil_text_processing(text):
    """Advanced Tamil text processing for natural speech"""

    print("üìù Processing Tamil text for natural speech...")

    try:
        # Remove unwanted characters but preserve Tamil script
        text = re.sub(r'[^\u0B80-\u0BFF\u0020-\u007E\s\n.,!?;:()"\'-]', '', text)

        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text)

        # Add natural pauses for punctuation
        text = re.sub(r'([.!?])', r'\1 <pause>', text) # Keeping pause tags for potential TTS engines
        text = re.sub(r'([,;:])', r'\1 <short_pause>', text) # Keeping short_pause tags


        # Split into sentences with better Tamil punctuation handling
        try:
            sentences = sent_tokenize(text, language='tamil') # Use Tamil tokenizer
        except LookupError:
             print("‚ö†Ô∏è NLTK punkt tokenizer not found for Tamil. Falling back to regex sentence splitting.")
             sentences = re.split(r'[.!?‡•§‡••]+', text) # Fallback regex split


        processed_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 5:
                # Add natural breathing points for long sentences
                if len(sentence) > 100:
                    # Split at commas or conjunctions
                    parts = re.split(r'(,|\b‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç\b|\b‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ\b|\b‡ÆÜ‡Æ©‡Ææ‡Æ≤‡Øç\b)', sentence)
                    # Rejoining with spaces, explicit breath tags are model-dependent
                    sentence = ' '.join(parts) # Removing explicit breath tags for now


                processed_sentences.append(sentence)

        print(f"‚úÖ Processed {len(processed_sentences)} sentences")
        return processed_sentences

    except Exception as e:
        print(f"‚ö†Ô∏è Text processing failed: {e}")
        return [text]

def detect_dialogue_and_characters(text):
    """Detect dialogue and assign characters automatically (Restored logic)"""

    print("üé≠ Detecting dialogue and characters...")

    # Simple dialogue detection patterns
    dialogue_patterns = [
        r'"([^"]+)"',  # Quoted text
        r'‚Äú([^‚Äù]+)‚Äù',  # Smart quotes
        r'(\w+)\s*[:-]\s*(.+)',  # Name: dialogue format
    ]

    segments = []
    character_count = 0
    character_map = {}

    lines = text.split('\n')

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # Default to narrator
        character = "narrator"
        is_dialogue = False

        for pattern in dialogue_patterns:
            matches = re.findall(pattern, line)
            if matches:
                is_dialogue = True

                # Try to detect speaker name
                speaker_match = re.search(r'(\w+)\s*[:-]', line)
                if speaker_match:
                    speaker_name = speaker_match.group(1)
                    if speaker_name not in character_map:
                        # Assign character based on order of appearance
                        available_characters = list(tts_system.character_voices.keys()) # Use available character keys
                        # Skip 'narrator' for assigned characters
                        available_characters.remove('narrator')
                        if character_count < len(available_characters):
                            character_key = available_characters[character_count]
                            character_map[speaker_name] = character_key
                            character_count += 1
                        else:
                            # Fallback if too many characters detected
                            character_map[speaker_name] = 'narrator'

                    character = character_map[speaker_name]
                    # Remove speaker name and separator from the text to be synthesized
                    line = re.sub(r'\w+\s*[:-]\s*', '', line).strip()

                else:
                    # Alternate between young male and female for unnamed dialogue
                    # Use keys that are likely to exist if basic characters are loaded
                    character = 'male_young' if len([s for s in segments if s['is_dialogue']]) % 2 == 0 else 'female_young'


                break # Found a dialogue pattern, process this line as dialogue


        segments.append({
            'text': line,
            'character': character,
            'is_dialogue': is_dialogue
        })

    print(f"‚úÖ Detected {len(segments)} segments with {len(character_map)} unique speakers identified (using character profiles)")
    return segments


# ============================================================================
# CELL 6: Advanced TTS Generation Functions (Restored logic)
# ============================================================================

def generate_natural_tamil_speech(text, character="narrator", reference_voice=None):
    """Generate natural-sounding Tamil speech with character voice"""

    if not text.strip():
        print("‚ùå Empty text provided")
        return None

    print(f"üé§ Generating speech for character: {tts_system.character_voices.get(character, {'name':character})['name']}") # Use character name from profile or key
    print(f"üìù Text: {text[:100]}...")

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
    temp_file.close()

    success = False

    # Method 1: Try XTTS v2 with voice cloning
    if tts_system.models.get('xtts_v2') and reference_voice and VOICE_CLONING_AVAILABLE:
        try:
            print("üöÄ Using XTTS v2 with voice cloning...")
            tts_system.models['xtts_v2'].tts_to_file(
                text=text,
                file_path=temp_file.name,
                speaker_wav=reference_voice,
                language="ta" # Ensure language is set to Tamil
            )
            success = True
            print("‚úÖ XTTS v2 generation successful!")
        except Exception as e:
            print(f"‚ö†Ô∏è XTTS v2 failed with cloning: {e}")

    # Method 2: Try XTTS v2 without cloning
    if not success and tts_system.models.get('xtts_v2'):
        try:
            print("üîÑ Using XTTS v2 without cloning...")
            # XTTS v2 might support speaker embeddings from a library included with XTTS itself,
            # or it might have default speakers. Using default for now.
            # Check XTTS v2 documentation for best practice without explicit cloning.
            # If XTTS v2 requires a speaker_wav even without cloning, this will fail.
            # As a simpler approach, just try generating text directly, XTTS might use a default voice.
            tts_system.models['xtts_v2'].tts_to_file(
                text=text,
                file_path=temp_file.name,
                language="ta"
                # Assuming no speaker_wav is needed for default voice
            )
            success = True
            print("‚úÖ XTTS v2 generation successful (without cloning)!")
        except Exception as e:
            print(f"‚ö†Ô∏è XTTS v2 failed without cloning: {e}")


    # Method 3: Try Bark TTS (Check for Tamil support in Bark models)
    if not success and tts_system.models.get('bark') and BARK_AVAILABLE:
        try:
            print("üîÑ Using Bark TTS...")
            # Bark requires text and an optional "voice_temp".
            # Voice temp can be a string like 'v2/en_speaker_9' or a numpy array for cloning.
            # For Tamil, finding a suitable Bark voice temp is key.
            # Bark's multilingual support might require specific prompts or voice temps.
            # A simple text generation attempt:
            # This is highly experimental for Tamil with Bark.
            # Let's try a neutral English voice template and see if Bark handles Tamil text.
            # Or, if there's a known Tamil Bark voice template, use it.
            # Using a generic multi-lingual prompt might be an alternative.
            # Example generic prompt structure (check Bark docs):
            # text_prompt = f"[l_ta]{text}[/l_ta]" # Example of language tag, verify with Bark docs
            # audio_array = generate_audio(text_prompt, history_prompt="v2/en_speaker_9") # Using an English template

            # Given uncertainty, stick to simpler generation if Bark installed:
            # Bark's generate_audio directly takes text and optional history_prompt.
            # A history_prompt can be a speaker sample or a predefined voice ID.
            # Without a specific Tamil Bark voice ID, generation might not be natural Tamil.
            # Let's try simple generation assuming it might default or attempt Tamil if the model supports it.
            audio_array = generate_audio(text)
            # Bark generates numpy array, need to save to WAV
            sf.write(temp_file.name, audio_array, SAMPLE_RATE)

            success = True
            print("‚úÖ Bark TTS generation successful (experimental)!")
        except Exception as e:
            print(f"‚ö†Ô∏è Bark TTS failed: {e}")

    # Method 4: Try YourTTS multi-speaker
    if not success and tts_system.models.get('yourtts') and ADVANCED_TTS_AVAILABLE:
        try:
            print("üîÑ Using YourTTS multi-speaker...")
            # YourTTS requires a speaker_wav for multi-speaker capability.
            # Without a specific reference voice, it might use a default or fail.
            # Using a default speaker from a multi-speaker model if possible:
            # Need to find a suitable speaker ID or sample for Tamil in YourTTS.
            # Check YourTTS documentation/available speaker IDs.
            # As a fallback, attempt generation without specific speaker, hoping for a default.
            tts_system.models['yourtts'].tts_to_file(
                text=text,
                file_path=temp_file.name,
                language="ta"
                # No speaker_wav provided, relies on default or model behavior
            )
            success = True
            print("‚úÖ YourTTS generation successful (without specific speaker)!")
        except Exception as e:
            print(f"‚ö†Ô∏è YourTTS failed: {e}")


    # Method 5: Try Tamil Tacotron2 (Specific Tamil Coqui model)
    if not success and tts_system.models.get('tamil_tacotron') and ADVANCED_TTS_AVAILABLE:
        try:
            print("üîÑ Using Tamil Tacotron2...")
            # This model is specifically for Tamil.
            tts_system.models['tamil_tacotron'].tts_to_file(
                text=text,
                file_path=temp_file.name
                # Language is implicit in the model name
            )
            success = True
            print("‚úÖ Tamil Tacotron2 generation successful!")
        except Exception as e:
            print(f"‚ö†Ô∏è Tamil Tacotron2 failed: {e}")


    # Method 6: Fallback to Google TTS
    if not success and tts_system.models.get('gtts') and GTTS_AVAILABLE:
        try:
            print("üîÑ Falling back to Google TTS...")
            # Convert to MP3 first, then to WAV
            temp_mp3 = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')
            temp_mp3.close()

            tts = gTTS(text=text, lang='ta', slow=False) # gTTS only has slow option for basic tone control
            tts.save(temp_mp3.name)

            # Convert MP3 to WAV
            audio = AudioSegment.from_mp3(temp_mp3.name)
            audio.export(temp_file.name, format="wav")

            os.unlink(temp_mp3.name)
            success = True
            print("‚úÖ Google TTS generation successful (fallback)!")
        except Exception as e:
            print(f"‚ùå Google TTS fallback failed: {e}")


    if success and os.path.exists(temp_file.name):
        return temp_file.name
    else:
        os.unlink(temp_file.name)
        print("‚ùå All TTS methods failed for this segment.")
        return None

def create_multi_character_audiobook(text, reference_voices=None, auto_detect_characters=True, tone_style="neutral", tone_intensity=1.0):
    """Create audiobook with multiple character voices"""

    print("üé¨ Creating multi-character Tamil audiobook...")
    print(f"üìñ Text length: {len(text)} characters")

    # Process text
    if auto_detect_characters:
        # Use the restored Tamil dialogue detection
        segments = detect_dialogue_and_characters(text)
    else:
        # Use the restored Tamil text processing
        processed_sentences = advanced_tamil_text_processing(text)
        segments = [{'text': s, 'character': 'narrator', 'is_dialogue': False}
                   for s in processed_sentences]

    if not segments:
        print("‚ùå No text segments found")
        return None, {} # Return empty dict on failure

    print(f"üìã Processing {len(segments)} segments...")

    audio_segments = []
    temp_files = []
    character_usage = {}

    try:
        for i, segment in enumerate(segments):
            if not segment['text'].strip():
                continue

            character = segment['character']
            text_content = segment['text']

            print(f"\nüé§ Segment {i+1}/{len(segments)}")
            print(f"üë§ Character: {tts_system.character_voices.get(character, {'name':character})['name']}")
            print(f"üìù Text: {text_content[:100]}...")

            # Track character usage
            character_usage[character] = character_usage.get(character, 0) + 1

            # Get reference voice for this character if available
            ref_voice = None
            if reference_voices and character in reference_voices:
                ref_voice = reference_voices[character]

            # Generate speech
            # Pass tone_style and tone_intensity, although impact depends on the active TTS model
            audio_path = generate_natural_tamil_speech(
                text_content,
                character=character,
                reference_voice=ref_voice
                # Tone/intensity are applied as post-processing effects if the TTS model doesn't support them
            )

            if audio_path and os.path.exists(audio_path):
                temp_files.append(audio_path)

                try:
                    # Load and process audio
                    audio_segment = AudioSegment.from_wav(audio_path)

                    # Apply character-specific effects (including tone/speed/pitch if implemented in apply_character_voice_effects)
                    character_profile = tts_system.character_voices.get(character, tts_system.character_voices['narrator'])
                    # Pass tone_style and intensity here for effects application logic
                    # Note: apply_character_voice_effects might need to be updated to handle these directly
                    # For now, the effects logic exists, but their effectiveness depends on the loaded model and audio quality.
                    audio_segment = apply_character_voice_effects(audio_segment, character_profile) # Assuming effects use profile directly


                    # Enhance audio quality
                    audio_segment = enhance_audio_quality(audio_segment)


                    duration = len(audio_segment) / 1000
                    print(f"‚è±Ô∏è Generated {duration:.2f}s for {character_profile['name']}")

                    audio_segments.append(audio_segment)

                    # Add appropriate pause
                    if i < len(segments) - 1:
                        next_character = segments[i + 1]['character']

                        # Longer pause for character changes
                        if auto_detect_characters and next_character != character: # Apply longer pause only if characters were auto-detected
                            pause_duration = 1000  # 1 second
                            print("‚è∏Ô∏è Character change - adding 1s pause")
                        else:
                            pause_duration = 500   # 0.5 second
                            print("‚è∏Ô∏è Adding 0.5s pause")

                        pause = AudioSegment.silent(duration=pause_duration)
                        audio_segments.append(pause)

                except Exception as e:
                    print(f"‚ùå Error processing segment {i}: {e}")
                    continue
            else:
                print(f"‚ùå Failed to generate audio for segment {i}. Skipping.")
                continue # Skip to next segment if generation failed


        if not audio_segments:
            print("‚ùå No audio segments were generated successfully")
            return None, {"error": "No audio segments could be generated."} # Return error if nothing was generated

        print(f"\nüîÑ Combining {len(audio_segments)} audio segments...")

        # Combine all segments
        final_audio = sum(audio_segments)
        total_duration = len(final_audio) / 1000

        print(f"‚è±Ô∏è Total audiobook duration: {total_duration:.2f} seconds")
        print(f"üë• Characters used: {list(character_usage.keys())}")


        # Final audio processing (keep this)
        try:
            print("üéöÔ∏è Applying final audio processing...")
            final_audio = normalize(final_audio)

            # Add subtle background ambience (keep this)
            print("üéµ Adding subtle background ambience...")
            bg_duration = len(final_audio)

            # Create layered background with multiple gentle tones
            bg1 = Sine(174).to_audio_segment(duration=bg_duration)  # Healing frequency
            bg2 = Sine(528).to_audio_segment(duration=bg_duration)  # Love frequency

            # Make very quiet
            bg1 = bg1 - 35
            bg2 = bg2 - 40

            # Layer backgrounds
            background = bg1.overlay(bg2)
            final_audio = background.overlay(final_audio) # Overlay background on speech

            print("‚úÖ Background ambience added")

        except Exception as e:
            print(f"‚ö†Ô∏è Final processing warning (background ambience): {e}")
            final_audio = final_audio # Use audio without background if it fails


        # Export final audiobook
        output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')
        print(f"üíæ Exporting final audiobook...")

        final_audio.export(output_file.name, format="mp3", bitrate="256k")

        # Verify output
        if os.path.exists(output_file.name) and os.path.getsize(output_file.name) > 0:
            file_size = os.path.getsize(output_file.name)
            print(f"‚úÖ Multi-character Tamil audiobook created!")
            print(f"üìÅ File size: {file_size:,} bytes")
            print(f"‚è±Ô∏è Duration: {total_duration:.2f} seconds")
            print(f"üë• Characters: {len(character_usage)} unique voices (using best available TTS)") # Updated note
        else:
            print("‚ùå Final export failed")
            return None, {"error": "Final audio export failed."}


        # Cleanup temp files
        for temp_file in temp_files:
            try:
                os.unlink(temp_file)
            except:
                pass

        return output_file.name, {
            "status": "success",
            "characters_used": character_usage,
            "duration_seconds": round(total_duration, 1),
            "file_size_bytes": file_size
        }

    except Exception as e:
        print(f"‚ùå Critical error during audiobook creation: {e}")
        import traceback
        traceback.print_exc()

        # Cleanup on error
        for temp_file in temp_files:
            try:
                os.unlink(temp_file)
            except:
                pass

        return None, {"error": f"Audiobook creation failed: {e}"}


# ============================================================================
# CELL 7: Enhanced PDF Processing (Keep these)
# ============================================================================

def extract_text_from_pdf_advanced(pdf_path):
    """Advanced PDF text extraction with Tamil support"""

    print(f"üìÑ Advanced PDF processing: {pdf_path}")
    text = ""

    try:
        # Method 1: pdfplumber with better Tamil handling
        with pdfplumber.open(pdf_path) as pdf:
            print(f"üìö Processing {len(pdf.pages)} pages...")

            for i, page in enumerate(pdf.pages):
                print(f"üìñ Page {i+1}/{len(pdf.pages)}")

                # Extract text with better encoding
                page_text = page.extract_text(
                    x_tolerance=2,
                    y_tolerance=2,
                    layout=True,
                    x_density=7.25,
                    y_density=13
                )

                if page_text:
                    # Clean extracted text
                    page_text = page_text.replace('\x00', '')  # Remove null bytes
                    page_text = re.sub(r'\n+', '\n', page_text)  # Normalize newlines
                    text += page_text + "\n\n"

        if text.strip():
            print(f"‚úÖ Extracted {len(text)} characters using pdfplumber")
            return text

    except Exception as e:
        print(f"‚ö†Ô∏è pdfplumber failed: {e}")

    try:
        # Method 2: PyMuPDF with Tamil encoding
        print("üîÑ Trying PyMuPDF with Tamil encoding...")

        doc = fitz.open(pdf_path)
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)

            # Get text with different extraction methods
            text_methods = [
                page.get_text(),
                page.get_text("text"),
                page.get_text("blocks"),
            ]

            page_text = ""
            for method_text in text_methods:
                if isinstance(method_text, list):
                    # Handle block format
                    for block in method_text: # Corrected variable name
                        if hasattr(block, '__len__') and len(block) > 4:
                             page_text += str(block[4]) + " "
                else:
                    page_text = str(method_text)
                    break

            if page_text.strip():
                text += page_text + "\n\n"

        doc.close()

        if text.strip():
            print(f"‚úÖ Extracted {len(text)} characters using PyMuPDF")
            return text

    except Exception as e:
        print(f"‚ùå PyMuPDF also failed: {e}")

    return "Error: Could not extract text from PDF. Please try a different PDF or convert to text file."


    # ============================================================================
# CELL 8 (continued): Main Processing Function + Case Mode Extraction (Restored Tamil Logic)
# ============================================================================

from transformers import pipeline

# Lightweight summarizer & NER pipelines (used for Case Mode and Quick Listen)
# Keep these as transformers and their models are generally compatible
try:
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
except Exception as e:
    print("‚ö†Ô∏è Summarizer not available locally, Quick Listen will fallback to first N chars.", e)
    summarizer = None

try:
    ner_pipe = pipeline("ner", model="dslim/bert-base-NER", grouped_entities=True)
except Exception as e:
    print("‚ö†Ô∏è NER pipeline not available locally.", e)
    ner_pipe = None

def simple_case_extraction(text):
    """Extract basic case fields heuristically for Case Mode"""
    # Try NER if available for names/dates; otherwise use regex heuristics.
    extracted = {
        "case_number": None,
        "parties": [],
        "judge": None,
        "verdict": None,
        "dates": []
    }
    try:
        # Case number pattern common forms
        m = re.search(r'\b(CA?SE|C\.NO|C No|Case No|CRIMINAL|C\.)[:\s]*([A-Za-z0-9\-\/\.]+)\b', text, re.IGNORECASE)
        if m:
            extracted["case_number"] = m.group(2)

        # Dates
        dates = re.findall(r'\b(?:\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4}|\d{1,2}\s(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s\d{2,4})\b', text, re.IGNORECASE)
        extracted["dates"] = dates

        # Try NER for PERSON (parties/judge)
        if ner_pipe:
            ner_out = ner_pipe(text[:2000])  # analyze first chunk
            persons = [e['word'] for e in ner_out if e['entity_group'] in ('PER','PERSON')]
            # heuristics: first two persons -> parties, last person labeled near verdict -> judge
            if persons:
                extracted["parties"] = persons[:2]
                if len(persons) > 2:
                    extracted["judge"] = persons[-1]

        # Try to find verdict lines (verdict/ordered/disposed)
        verdict_match = re.search(r'(verdict|ordered|disposed|hold|dismissed|allowed|acquitted|convicted|judgment|judgement)[\s\S]{0,120}', text, re.IGNORECASE)
        if verdict_match:
            extracted["verdict"] = verdict_match.group(0).strip()

    except Exception as e:
        print("‚ö†Ô∏è Case extraction heuristic failed:", e)

    return extracted

def process_tamil_multicharacter_audiobook( # Restored original function name
    file=None,
    text_input="",
    auto_detect_characters=True,
    character_count=3,
    tone_style="neutral",
    tone_intensity=1.0,
    reference_voice_files=None,
    quick_listen=False,
    mode="story"   # "story" or "case"
):
    """
    Main entry function invoked by Gradio.
    - file: uploaded PDF or txt file (temp file path object from Gradio)
    - text_input: pasted text
    - auto_detect_characters: boolean, whether to auto detect dialogue and characters
    - character_count: int, maximum number of auto-assigned characters
    - tone_style: string, one of narrator tone options
    - tone_intensity: float, intensity multiplier
    - reference_voice_files: dict-like or list of uploaded files for cloning (optional)
    - quick_listen: if True produce a short summary audio (use summarizer)
    - mode: "story" or "case"
    Returns: (audio_file_path or None, status_message or dict)
    """
    # --- prepare text ---
    extracted_text = ""
    try:
        if file:
            # file is an UploadedFile-like object or path
            # Gradio sometimes passes a tempfile path (string)
            file_path = file.name if hasattr(file, "name") else file
            if isinstance(file_path, str) and file_path.lower().endswith('.pdf'):
                extracted_text = extract_text_from_pdf_advanced(file_path)
            else:
                # try reading as txt
                try:
                    with open(file_path, "r", encoding="utf-8") as fh:
                        extracted_text = fh.read()
                except Exception:
                    # try pdfplumber fallback
                    extracted_text = extract_text_from_pdf_advanced(file_path)
        else:
            extracted_text = text_input or ""
    except Exception as e:
        return None, {"error": f"Failed to read input file/text: {e}"}

    if not extracted_text or extracted_text.startswith("Error:"):
        return None, {"error": "No text could be extracted. Please check the PDF or paste text manually."}

    # --- Case Mode: extract structured case details and produce a short narrated summary ---
    if mode == "case":
        case_info = simple_case_extraction(extracted_text)
        # create a short summary for quick listen if requested
        summary_text = extracted_text # Default to full text if summarizer not available or quick_listen is False
        if quick_listen and summarizer:
            try:
                summary_text = summarizer(extracted_text, max_length=200, min_length=40, do_sample=False)[0]['summary_text']
                print("‚úÖ Text summarized for Case Quick Listen.")
            except Exception as e:
                print(f"‚ö†Ô∏è Summarization failed for Case Quick Listen: {e}. Falling back to first 800 characters.")
                summary_text = extracted_text[:800]  # fallback


        # Narrate summary using narrator voice (will attempt multi-character if enabled)
        audio_file = None
        try:
            # Generate audio for case summary - can still use auto-detection but it might fall back to single narrator
            # Force single character/narrator for case summary as it's more appropriate
            out = create_multi_character_audiobook(summary_text, reference_voices=None, auto_detect_characters=False, tone_style=tone_style, tone_intensity=tone_intensity)
            if out and isinstance(out, tuple):
                audio_file, character_usage = out
            else:
                audio_file = out # Should be the file path

        except Exception as e:
            return None, {"error": f"Case Mode TTS failed: {e}", "case_extracted": case_info}

        # Ensure audio_file is a path string before returning
        if isinstance(audio_file, (list, tuple)):
             audio_file = audio_file[0] if audio_file else None


        # Combine case info with audio info
        info_dict = {
            "case_extracted": case_info,
            "audio_info": {"duration_seconds": 0, "file_size_bytes": 0, "characters_used": {}}
        }
        if out and isinstance(out, tuple):
            info_dict["audio_info"]["duration_seconds"] = round(AudioSegment.from_file(audio_file).duration_seconds, 1) if audio_file and os.path.exists(audio_file) else 0
            info_dict["audio_info"]["file_size_bytes"] = os.path.getsize(audio_file) if audio_file and os.path.exists(audio_file) else 0
            info_dict["audio_info"]["characters_used"] = character_usage
            info_dict["status"] = "success" if audio_file else "audio_generation_failed"

        elif out is not None: # Audio file was generated but not tuple
             info_dict["audio_info"]["duration_seconds"] = round(AudioSegment.from_file(audio_file).duration_seconds, 1) if audio_file and os.path.exists(audio_file) else 0
             info_dict["audio_info"]["file_size_bytes"] = os.path.getsize(audio_file) if audio_file and os.path.exists(audio_file) else 0
             info_dict["audio_info"]["characters_used"] = {'narrator': 1} # Assume narrator for single file output
             info_dict["status"] = "success" if audio_file else "audio_generation_failed"
        else:
             info_dict["status"] = "audio_generation_failed"
             info_dict["error"] = "Audio generation failed."

        return audio_file, info_dict


    # --- Story Mode: Generate audiobook ---
    # Quick listen for story mode is handled by summarizing the input text before sending to create_multi_character_audiobook
    working_text = extracted_text
    if quick_listen:
        if summarizer:
            try:
                summary_text = summarizer(extracted_text, max_length=180, min_length=30, do_sample=False)[0]['summary_text']
                working_text = summary_text
                print("‚úÖ Text summarized for Story Quick Listen.")
            except Exception as e:
                print(f"‚ö†Ô∏è Summarization failed for Story Quick Listen: {e}. Falling back to first 1200 characters.")
                # fallback: take first N chars
                working_text = extracted_text[:1200]
        else:
            print("‚ö†Ô∏è Summarizer not available. Falling back to first 1200 characters for Quick Listen.")
            working_text = extracted_text[:1200]


    # Generate audiobook (will attempt multi-character if enabled and models load)
    out = create_multi_character_audiobook(
        working_text,
        reference_voices=reference_voice_files, # Pass reference voices
        auto_detect_characters=auto_detect_characters, # Pass auto-detect flag
        tone_style=tone_style, # Pass tone style
        tone_intensity=tone_intensity # Pass tone intensity
    )
    audio_file_path, info_dict = (None, {})
    if isinstance(out, tuple):
        audio_file_path, info_dict = out
    else:
        audio_file_path = out # Should be the file path
        info_dict = {"status": "audio_generation_failed" if audio_file_path is None else "success"} # Default info if not a tuple


    if not audio_file_path:
        return None, {"error": info_dict.get("error", "Audio generation failed.")}


    # Return success info and path
    # Ensure info_dict has necessary keys even on success
    if "characters_used" not in info_dict: info_dict["characters_used"] = {}
    if "duration_seconds" not in info_dict: info_dict["duration_seconds"] = round(AudioSegment.from_file(audio_file_path).duration_seconds, 1) if os.path.exists(audio_file_path) else 0
    if "file_size_bytes" not in info_dict: info_dict["file_size_bytes"] = os.path.getsize(audio_file_path) if os.path.exists(audio_file_path) else 0


    return audio_file_path, info_dict


# ============================================================================
# CELL 9: Gradio UI - Story Mode + Case Mode Tabbed Interface (Restored Tamil Focus)
# ============================================================================

import gradio as gr
from pathlib import Path

def human_readable_size(path):
    try:
        s = os.path.getsize(path)
        for unit in ['bytes','KB','MB','GB']:
            if s < 1024.0:
                return f"{s:.1f} {unit}"
            s /= 1024.0
        return f"{s:.1f} TB"
    except:
        return "Unknown"

with gr.Blocks(title="Advanced Tamil EchoVerse") as demo: # Restored title
    gr.Markdown("## üéß Advanced Tamil EchoVerse ‚Äî Multi-character Natural Voice Generation (Attempting Advanced Models)") # Restored markdown
    gr.Markdown("‚ö†Ô∏è **Note:** This notebook attempts to install advanced Tamil TTS models (like Coqui TTS, Bark) for natural multi-character voices. If these models fail to install due to environment compatibility, it will fall back to Google TTS (gTTS), which may not provide the same level of naturalness or multi-character support.") # Updated explanation

    with gr.Tab("Story Mode"):
        with gr.Row():
            with gr.Column(scale=3):
                upload = gr.File(label="Upload PDF or TXT (Tamil)", file_types=[".pdf", ".txt"]) # Restored label
                text_box = gr.Textbox(label="Or paste Tamil text here", lines=12, placeholder="Paste your Tamil story or chapter...") # Restored label
            with gr.Column(scale=1):
                auto_detect = gr.Checkbox(value=True, label="Auto-detect characters & dialogues") # Restored checkbox
                character_count = gr.Slider(minimum=1, maximum=8, step=1, value=3, label="Max characters to assign (if advanced models load)") # Restored slider, updated info
                tone = gr.Dropdown(["neutral","happy","sad","dramatic","mysterious","calm","excited","angry"], value="neutral", label="Tone style (depends on model support)") # Restored options, updated info
                intensity = gr.Slider(minimum=0.1, maximum=2.0, step=0.1, value=1.0, label="Tone intensity (depends on model support)") # Restored slider, updated info
                quick = gr.Checkbox(value=False, label="Quick Listen (summary)") # Restored checkbox
                ref_voice = gr.File(label="(Optional) Upload reference voice files (one or more WAV/MP3) for cloning (if models load)", file_count="multiple") # Restored file upload, updated info
                gen_btn = gr.Button("üé¨ Create Tamil Audiobook") # Restored label
        out_audio = gr.Audio(label="Generated Audiobook", type="filepath") # Restored label
        out_info = gr.JSON(label="Generation Info / Status") # Restored label

    with gr.Tab("Case Mode (Basic Extraction)"): # Restored tab name
        with gr.Row():
            with gr.Column(scale=3):
                case_upload = gr.File(label="Upload Case PDF / TXT (Tamil)", file_types=[".pdf", ".txt"]) # Restored label
                case_text = gr.Textbox(label="Or paste case text (Tamil)", lines=12) # Restored label
            with gr.Column(scale=1):
                case_quick = gr.Checkbox(value=True, label="Narrate Extracted Details", info="If checked, a summary of extracted case details will be narrated (single voice).") # Updated label and info
                case_gen = gr.Button("‚öñÔ∏è Extract & Narrate Case Details") # Restored label
        case_audio = gr.Audio(label="Case Details Audio Summary", type="filepath") # Restored label
        case_info = gr.JSON(label="Extracted Case Details & Audio Info") # Updated label

    # Hook up Story Mode button (Restored original function call and inputs/outputs)
    def on_generate(upload, text_box, auto_detect, character_count, tone, intensity, quick, ref_voice):
        file_arg = upload
        text_arg = text_box
        ref_files = list(ref_voice) if ref_voice else None
        audio_path, info = process_tamil_multicharacter_audiobook(
            file=file_arg,
            text_input=text_arg,
            auto_detect_characters=auto_detect,
            character_count=character_count,
            tone_style=tone,
            tone_intensity=float(intensity),
            reference_voice_files=ref_files,
            quick_listen=quick,
            mode="story"
        )
        if audio_path:
            return audio_path, info
        else:
            return None, info

    gen_btn.click(
        on_generate,
        inputs=[upload, text_box, auto_detect, character_count, tone, intensity, quick, ref_voice], # Restored inputs
        outputs=[out_audio, out_info] # Restored outputs
    )

    # Hook up Case Mode button (Restored original function call and inputs/outputs)
    def on_case_generate(case_upload, case_text, case_quick):
        file_arg = case_upload
        text_arg = case_text
        # For case mode, we don't use multi-character logic or external reference voices in the main call
        audio_path, info = process_tamil_multicharacter_audiobook(
            file=file_arg,
            text_input=text_arg,
            auto_detect_characters=False, # Force single narrator for case
            character_count=1, # Only 1 character matters for case
            tone_style="neutral", # Default tone for case
            tone_intensity=1.0, # Default intensity for case
            reference_voice_files=None, # No external reference voices for case
            quick_listen=case_quick, # Pass case_quick for summarization
            mode="case"
        )
        # The info dictionary for case mode now includes both case_extracted and audio_info
        case_details = info.get("case_extracted", {})
        audio_info = {k: v for k, v in info.items() if k != "case_extracted" and k != "error"} # Separate audio info, exclude error
        status = info.get("status", "unknown")
        error = info.get("error", None)

        # Combine case details and audio info into a single dict for the info output
        combined_info = {"case_extracted": case_details, "audio_info": audio_info, "status": status}
        if error:
            combined_info["error"] = error


        if audio_path:
            return audio_path, combined_info # Return audio path and combined info
        else:
             # If audio generation failed, return None audio and the combined info
            return None, combined_info


    case_gen.click(
        on_case_generate,
        inputs=[case_upload, case_text, case_quick], # Restored inputs
        outputs=[case_audio, case_info] # Restored outputs
    )


    gr.Markdown("---")
    gr.Markdown("**Notes:** This Colab demo attempts to use advanced TTS models for natural multi-character Tamil speech. If these models fail to install (check the output above), it will fall back to Google TTS (gTTS), which provides a single voice and less natural output. Performance and features depend heavily on which models successfully load.") # Updated notes

# Launch the interface (in Colab, share=True allows public link)
demo.launch(share=True)

