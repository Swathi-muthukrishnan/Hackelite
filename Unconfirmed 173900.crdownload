# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BtD_UV97RTB3sXKtoBQaQM7jeM8uQZX1
"""



"""# Task
Create a web application using Gradio that takes a story as input (either typed or uploaded as a PDF), analyzes it to identify characters, dialogue, emotional cues, and potential sound effects (including animal sounds), and then generates an emotional voiceover with multiple natural-sounding character voices and appropriate background noise.

## Refine text processing

### Subtask:
Improve character and dialogue detection, and potentially add scene analysis to identify emotional cues and required sound effects.

**Reasoning**:
The first step is to enhance the `detect_characters_and_dialogue` function to improve character and dialogue detection, and also add scene analysis to identify emotional cues and sound effects. This involves modifying the existing function to use more robust pattern matching and incorporating logic for identifying emotional words and sound effect mentions. The structure of the output `scenes` also needs to be updated to include these new elements.
"""

import spacy
import re

# Load a pre-trained English model for NER and linguistic features
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    # If the model is not downloaded, download it
    print("Downloading spaCy model 'en_core_web_sm'...")
    from spacy.cli import download
    download("en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

def detect_characters_and_dialogue(text):
    """
    Improved heuristics using spaCy for NER and pattern matching.
    Also includes basic scene analysis for emotions and sound effects.
    Returns: characters list and structured scenes (list of {scene_id, lines, emotional_cues, sound_effects})
    """
    doc = nlp(text)

    # Attempt 1: Explicit character: Dialogue pattern
    explicit_lines = []
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    for l in lines:
        m = re.match(r'^([A-Z][A-Za-z0-9 _\-]{1,30}):\s*(.+)$', l)
        if m:
            explicit_lines.append({'character': m.group(1).title(), 'text': m.group(2), 'emotional_cues': [], 'sound_effects': []})

    if explicit_lines:
        characters = list(sorted({l['character'] for l in explicit_lines}))
        # Simple scene structure for explicit format
        scenes = [{'scene_id': 0, 'lines': explicit_lines, 'scene_summary': '', 'emotional_cues': [], 'sound_effects': []}]
        # Basic scene-level analysis for explicit case
        scene_text = " ".join([l['text'] for l in explicit_lines])
        scenes[0]['emotional_cues'] = analyze_emotional_cues(scene_text)
        scenes[0]['sound_effects'] = analyze_sound_effects(scene_text)
        return characters, scenes

    # Attempt 2: NER and frequency for character detection in non-explicit text
    characters = []
    person_counts = {}
    for ent in doc.ents:
        if ent.label_ == "PERSON":
            person_counts[ent.text] = person_counts.get(ent.text, 0) + 1

    # Consider names appearing at least twice as characters (heuristic)
    characters = [name for name, count in person_counts.items() if count >= 2]
    characters = sorted(characters)

    # Fallback: If no characters detected by NER, use the old Titlecase heuristic
    if not characters:
         tokens = re.findall(r"\b([A-Z][a-z]{2,20})\b", text)
         freq = {}
         for t in tokens:
             freq[t] = freq.get(t,0) + 1
         cand = sorted(freq.items(), key=lambda x: -x[1])[:4]
         characters = [c for c,_ in cand]

    # Split text into potential scenes (using chunks for now)
    chunks = chunk_text_by_scene(text, approx_words=200)
    scenes = []

    # Simple dialogue attribution and analysis for non-explicit format
    current_char_idx = 0
    for i, chunk in enumerate(chunks):
        scene_lines = []
        # Split by sentences for pseudo-lines
        sents = re.split(r'(?<=[.!?])\s+', chunk)
        scene_emotional_cues = []
        scene_sound_effects = []

        for j, sent in enumerate(sents):
            sent = sent.strip()
            if not sent:
                continue

            # Simple round-robin character attribution for non-explicit cases
            char = characters[current_char_idx % max(1,len(characters))]
            current_char_idx += 1

            # Analyze sentence for emotional cues and sound effects
            line_emotional_cues = analyze_emotional_cues(sent)
            line_sound_effects = analyze_sound_effects(sent)

            scene_lines.append({'character': char, 'text': sent, 'emotional_cues': line_emotional_cues, 'sound_effects': line_sound_effects})
            scene_emotional_cues.extend(line_emotional_cues)
            scene_sound_effects.extend(line_sound_effects)


        scenes.append({'scene_id': i, 'lines': scene_lines, 'scene_summary': ' '.join(sents[:2]),
                       'emotional_cues': list(set(scene_emotional_cues)), # unique cues per scene
                       'sound_effects': list(set(scene_sound_effects))}) # unique sfx per scene

    return characters, scenes

def analyze_emotional_cues(text):
    """Basic analysis to find words indicating emotions."""
    emotional_words = ['happy', 'sad', 'angry', 'excited', 'scared', 'joy', 'fear', 'rage', 'calm', 'nervous']
    cues = [word for word in emotional_words if re.search(r'\b' + word + r'\b', text, re.IGNORECASE)]
    return cues

def analyze_sound_effects(text):
    """Basic analysis to find mentions of sound effects, including animal sounds."""
    sound_keywords = ['barked', 'howled', 'meowed', 'roared', 'hissed', 'wind', 'rain', 'thunder', 'door creaked', 'footsteps', 'silence', 'whispered', 'shouted', 'laughed', 'cried']
    effects = [keyword for keyword in sound_keywords if re.search(r'\b' + keyword + r'\b', text, re.IGNORECASE)]
    return effects

# Update the process_pdf_and_prepare_script to use the new detection function
def process_pdf_and_prepare_script(uploaded_file_path):
    full_text, pages = extract_text_from_pdf_fp(uploaded_file_path)
    characters, scenes = detect_characters_and_dialogue(full_text)
    char_map = map_characters_to_voices(characters)
    return {'full_text': full_text, 'pages': pages, 'characters': characters, 'scenes': scenes, 'char_map': char_map}

"""## Integrate advanced tts models

### Subtask:
Replace the placeholder `synthesize_line_tts_backend` function with actual implementations for models like Bark, Tortoise, or other suitable models that support multiple speakers and emotional prosody. This may involve installing additional libraries and handling model loading.

**Reasoning**:
Install the necessary libraries for Bark and Tortoise TTS models.
"""

# Install Bark (suno-ai/bark) and its dependencies
!pip install --quiet git+https://github.com/suno-ai/bark.git scipy transformers accelerate

# Install Tortoise-TTS (neonbjb/tortoise-tts) and its dependencies
!pip install --quiet git+https://github.com/neonbjb/tortoise-tts.git
!pip install --quiet transformers==4.19.0 # Tortoise often works best with a specific transformers version
!pip install --quiet einops rotary_embedding_torch

"""**Reasoning**:
The previous installation attempt failed for Tortoise-TTS and tokenizers due to dependency conflicts. I will try installing Tortoise-TTS with a specific commit that might resolve the dependency issues, and also install tokenizers separately before other packages.


"""

# Install tokenizers separately first
!pip install --quiet tokenizers

# Install Tortoise-TTS from a known working commit (example commit hash)
# Replace with a commit hash known to be compatible if needed
!pip install --quiet git+https://github.com/neonbjb/tortoise-tts.git@e3e37a0c5cb57478282642f53460d91e89930ae7

# Reinstall transformers with a version that might be more compatible with Tortoise
!pip install --quiet transformers==4.19.0

# Ensure other potential dependencies for Tortoise are present
!pip install --quiet einops rotary_embedding_torch

"""**Reasoning**:
The installation of Tortoise-TTS and tokenizers is still failing. It seems there might be an issue with fetching the specific commit or building the tokenizers wheel. Given the difficulties with Tortoise, I will prioritize implementing Bark first, as its installation seemed more successful in the initial attempt. I will then modify the `synthesize_line_tts_backend` function to include the Bark implementation and load the Bark model.


"""

import torch
from transformers import BarkModel, AutoTokenizer

# Check for GPU and set device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load Bark model and tokenizer
# You might need to specify a different model name depending on availability and requirements
# For example, "suno/bark" is the base model
try:
    bark_tokenizer = AutoTokenizer.from_pretrained("suno/bark")
    bark_model = BarkModel.from_pretrained("suno/bark").to(device)
    print("Bark model and tokenizer loaded successfully.")
except Exception as e:
    print(f"Error loading Bark model: {e}")
    bark_tokenizer = None
    bark_model = None

# Update the synthesize_line_tts_backend function
def synthesize_line_tts_backend(character, text, emotion='neutral', backend='gTTS', backend_params=None):
    """Synthesize a single line to an audio file path.
    backend: 'gTTS', 'bark', 'tortoise', 'parler' etc.
    """
    backend_params = backend_params or {}
    if backend == 'gTTS':
        print(f"Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)
    elif backend == 'bark' and bark_model is not None and bark_tokenizer is not None:
        print(f"Using Bark for character {character} with emotion '{emotion}': {text[:50]}...")
        # Bark implementation
        # This is a basic implementation; more advanced usage might involve speaker prompts
        try:
            # You can try to inject emotion into the text for Bark
            text_prompt = f"[{emotion}] {text}" if emotion != 'neutral' else text

            inputs = bark_tokenizer(text_prompt, return_tensors="pt").to(device)
            # Generate audio
            with torch.no_grad():
                speech_output = bark_model.generate(**inputs, do_sample=True, fine_tuned=False)

            # Convert tensor to numpy array
            audio_array = speech_output[0].cpu().numpy()

            # Save to a temporary WAV file (Bark outputs raw audio)
            tmpfile_wav = f'/content/{uuid.uuid4().hex}.wav'
            sf.write(tmpfile_wav, audio_array, bark_model.generation_config.sample_rate)

            # Convert WAV to MP3 for consistency with the rest of the pipeline
            tmpfile_mp3 = f'/content/{uuid.uuid4().hex}.mp3'
            AudioSegment.from_wav(tmpfile_wav).export(tmpfile_mp3, format="mp3")

            # Clean up the temporary WAV file
            os.remove(tmpfile_wav)

            return tmpfile_mp3
        except Exception as e:
            print(f"Error synthesizing with Bark: {e}")
            # Fallback to gTTS if Bark fails
            print(f"Falling back to gTTS for character {character}.")
            return synthesize_with_gtts(text)

    elif backend == 'tortoise':
        # Tortoise implementation placeholder - will add if installation issues are resolved
        print(f"Tortoise backend selected but not implemented. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(f"[{character} - {emotion}] {text}")
    else:
        print(f"Unknown backend '{backend}'. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)

"""**Reasoning**:
The Bark model was loaded successfully, although it's using the CPU. The `synthesize_line_tts_backend` function has been updated to include the Bark implementation. The next step is to verify the integration by attempting to synthesize a scene using the 'bark' backend and confirming that the audio is generated correctly. I will modify the `on_synthesize_scene` function call in the Gradio interface to use the 'bark' backend for testing.


"""

# Update the Gradio interface build function to set the default model choice to 'bark' for testing
def build_gradio_interface():
    with gr.Blocks() as demo:
        gr.Markdown('# PDF → Emotional Audioplay')
        with gr.Row():
            pdf_in = gr.File(label='Upload PDF', file_types=['.pdf'])
            # Change default value to 'bark' for testing
            model_choice = gr.Radio(choices=['gTTS (fast, low quality)','bark (expressive)','tortoise (high quality)','parler/xtts (emotion)'], value='bark (expressive)', label='TTS Backend')
        out_text = gr.Textbox(label='Extracted text (preview)', lines=10)
        characters_json = gr.JSON(label='Detected Characters & Defaults')
        scene_dropdown = gr.Dropdown(label='Scenes', choices=[], value=None)
        scene_player = gr.Audio(label='Scene audio')
        combined_player = gr.Audio(label='Combined audio')
        music_upload = gr.File(label='Background music (optional)', file_types=['.mp3','.wav'])
        intensity = gr.Slider(minimum=0.0, maximum=1.0, value=0.3, label='Music intensity')
        generate_btn = gr.Button('Generate Script & Characters')
        synth_scene_btn = gr.Button('Generate Scene Audio')
        synth_full_btn = gr.Button('Generate Full Audio')

        state = gr.State({})

        def on_generate(pdf_file, model_choice_val):
            if not pdf_file:
                return 'Upload a PDF to begin.', {}, [], None
            tmp_path = pdf_file.name
            result = process_pdf_and_prepare_script(tmp_path)
            # scene labels
            scenes = result['scenes']
            scene_labels = [f"Scene {s['scene_id']}: {(' '.join(s['scene_summary']))[:80]}" for s in scenes]
            return result['full_text'][:4000], result['char_map'], scene_labels, result

        def on_synthesize_scene(selected_scene_label, state_obj, music_file, intensity_val, backend_choice):
            if not state_obj:
                return None, None
            scenes = state_obj['scenes']
            # map label back to scene index
            idx = int(selected_scene_label.split()[1].strip(':'))
            scene = scenes[idx]
            # Ensure backend is correctly passed, stripping the user-friendly part
            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None
            out = synthesize_scene(scene, state_obj['char_map'], backend=backend, music_path=music_path, intensity=intensity_val)
            return out, state_obj

        def on_synthesize_full(state_obj, music_file, intensity_val, backend_choice):
            if not state_obj:
                return None, []
            # Ensure backend is correctly passed, stripping the user-friendly part
            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None
            out, scene_files = synthesize_full(state_obj['scenes'], state_obj['char_map'], backend=backend, music_path=music_path, intensity=intensity_val)
            return out, scene_files

        generate_btn.click(on_generate, inputs=[pdf_in, model_choice], outputs=[out_text, characters_json, scene_dropdown, state])
        synth_scene_btn.click(on_synthesize_scene, inputs=[scene_dropdown, state, music_upload, intensity, model_choice], outputs=[scene_player, state])
        synth_full_btn.click(on_synthesize_full, inputs=[state, music_upload, intensity, model_choice], outputs=[combined_player, gr.Dataset()])

    return demo

# Rebuild the demo interface
demo = build_gradio_interface()

# You would typically run demo.launch() here, but since we are in a notebook
# environment focused on the function implementation, we won't launch the full UI yet.
# The next step will involve testing the synthesis function directly.

"""**Reasoning**:
The previous command failed because the `gr` object from the `gradio` library was not in the current scope. This is because the cell where `gradio` was imported was executed earlier in the notebook's history, but the current execution context doesn't have access to it. I need to re-import `gradio` to make the `gr` object available.


"""

import gradio as gr

# Update the Gradio interface build function to set the default model choice to 'bark' for testing
def build_gradio_interface():
    with gr.Blocks() as demo:
        gr.Markdown('# PDF → Emotional Audioplay')
        with gr.Row():
            pdf_in = gr.File(label='Upload PDF', file_types=['.pdf'])
            # Change default value to 'bark' for testing
            model_choice = gr.Radio(choices=['gTTS (fast, low quality)','bark (expressive)','tortoise (high quality)','parler/xtts (emotion)'], value='bark (expressive)', label='TTS Backend')
        out_text = gr.Textbox(label='Extracted text (preview)', lines=10)
        characters_json = gr.JSON(label='Detected Characters & Defaults')
        scene_dropdown = gr.Dropdown(label='Scenes', choices=[], value=None)
        scene_player = gr.Audio(label='Scene audio')
        combined_player = gr.Audio(label='Combined audio')
        music_upload = gr.File(label='Background music (optional)', file_types=['.mp3','.wav'])
        intensity = gr.Slider(minimum=0.0, maximum=1.0, value=0.3, label='Music intensity')
        generate_btn = gr.Button('Generate Script & Characters')
        synth_scene_btn = gr.Button('Generate Scene Audio')
        synth_full_btn = gr.Button('Generate Full Audio')

        state = gr.State({})

        def on_generate(pdf_file, model_choice_val):
            if not pdf_file:
                return 'Upload a PDF to begin.', {}, [], None
            tmp_path = pdf_file.name
            result = process_pdf_and_prepare_script(tmp_path)
            # scene labels
            scenes = result['scenes']
            scene_labels = [f"Scene {s['scene_id']}: {(' '.join(s['scene_summary']))[:80]}" for s in scenes]
            return result['full_text'][:4000], result['char_map'], scene_labels, result

        def on_synthesize_scene(selected_scene_label, state_obj, music_file, intensity_val, backend_choice):
            if not state_obj:
                return None, None
            scenes = state_obj['scenes']
            # map label back to scene index
            idx = int(selected_scene_label.split()[1].strip(':'))
            scene = scenes[idx]
            # Ensure backend is correctly passed, stripping the user-friendly part
            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None
            out = synthesize_scene(scene, state_obj['char_map'], backend=backend, music_path=music_path, intensity=intensity_val)
            return out, state_obj

        def on_synthesize_full(state_obj, music_file, intensity_val, backend_choice):
            if not state_obj:
                return None, []
            # Ensure backend is correctly passed, stripping the user-friendly part
            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None
            out, scene_files = synthesize_full(state_obj['scenes'], state_obj['char_map'], backend=backend, music_path=music_path, intensity=intensity_val)
            return out, scene_files

        generate_btn.click(on_generate, inputs=[pdf_in, model_choice], outputs=[out_text, characters_json, scene_dropdown, state])
        synth_scene_btn.click(on_synthesize_scene, inputs=[scene_dropdown, state, music_upload, intensity, model_choice], outputs=[scene_player, state])
        synth_full_btn.click(on_synthesize_full, inputs=[state, music_upload, intensity, model_choice], outputs=[combined_player, gr.Dataset()])

    return demo

# Rebuild the demo interface
demo = build_gradio_interface()

# You would typically run demo.launch() here, but since we are in a notebook
# environment focused on the function implementation, we won't launch the full UI yet.
# The next step will involve testing the synthesis function directly.

"""**Reasoning**:
The Gradio interface definition is now complete and the Bark model is loaded. To verify the Bark implementation in `synthesize_line_tts_backend`, I need to simulate the call to this function with sample data representing a scene and character map. I will create dummy data for a scene and char_map and call `synthesize_scene` (which internally calls `synthesize_line_tts_backend`) with the 'bark' backend.


"""

# Create dummy data for testing synthesis
test_scene = {
    'scene_id': 99,
    'lines': [
        {'character': 'Alice', 'text': "Hello there! I'm feeling quite happy today.", 'emotional_cues': ['happy'], 'sound_effects': []},
        {'character': 'Bob', 'text': "Oh really? I'm a bit worried about the weather.", 'emotional_cues': ['worried'], 'sound_effects': ['rain']},
        {'character': 'Alice', 'text': "Don't worry, I'm sure it will be fine!", 'emotional_cues': [], 'sound_effects': []}
    ],
    'scene_summary': 'Alice and Bob discuss the weather.',
    'emotional_cues': ['happy', 'worried'],
    'sound_effects': ['rain']
}

test_char_map = {
    'Alice': {'voice': 'female_default', 'gender':'female', 'emotion':'neutral'},
    'Bob': {'voice': 'male_default', 'gender':'male', 'emotion':'neutral'}
}

# Synthesize the test scene using the 'bark' backend
print("Synthesizing test scene with Bark...")
try:
    synthesized_audio_path = synthesize_scene(test_scene, test_char_map, backend='bark')
    print(f"Synthesis complete. Output file: {synthesized_audio_path}")

    # Optional: Load and check the synthesized audio (requires soundfile and numpy)
    # audio_data, samplerate = sf.read(synthesized_audio_path)
    # print(f"Synthesized audio shape: {audio_data.shape}, Sample rate: {samplerate}")

except Exception as e:
    print(f"An error occurred during Bark synthesis test: {e}")

"""**Reasoning**:
The previous command failed because the `synthesize_scene` function was not defined in the current scope. Although it was defined in an earlier code block in the notebook's history, the current execution context doesn't have access to it. I need to re-include the definition of `synthesize_scene` and its dependencies like `synthesize_line_tts_backend`, `mix_with_background`, and `AudioSegment` to make them available for execution.


"""

import torch
from transformers import BarkModel, AutoTokenizer
from pydub import AudioSegment
import soundfile as sf
import numpy as np
import os
import uuid
from gtts import gTTS # Also need gTTS for the fallback

# Check for GPU and set device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load Bark model and tokenizer (re-loading in case the kernel state was reset)
try:
    bark_tokenizer = AutoTokenizer.from_pretrained("suno/bark")
    bark_model = BarkModel.from_pretrained("suno/bark").to(device)
    print("Bark model and tokenizer loaded successfully.")
except Exception as e:
    print(f"Error loading Bark model: {e}")
    bark_tokenizer = None
    bark_model = None

# Fallback TTS using gTTS (re-include as it's a dependency)
def synthesize_with_gtts(text, lang='en'):
    tts = gTTS(text=text, lang=lang)
    tmpfile = f'/content/{uuid.uuid4().hex}.mp3'
    tts.save(tmpfile)
    return tmpfile

# Mix background music with ducking (re-include as it's a dependency)
def mix_with_background(speech_path, music_path=None, intensity=0.3):
    speech = AudioSegment.from_file(speech_path)
    if not music_path:
        return speech
    music = AudioSegment.from_file(music_path)
    # make music match duration
    if len(music) < len(speech):
        # loop music
        times = int(len(speech) / len(music)) + 1
        music = music * times
    music = music[:len(speech)]
    # duck music
    music = music - (10 * (1 - intensity))  # simple attenuation
    mixed = speech.overlay(music)
    outpath = f'/content/{uuid.uuid4().hex}_mixed.mp3'
    mixed.export(outpath, format='mp3')
    return outpath


# Update the synthesize_line_tts_backend function (re-include)
def synthesize_line_tts_backend(character, text, emotion='neutral', backend='gTTS', backend_params=None):
    """Synthesize a single line to an audio file path.
    backend: 'gTTS', 'bark', 'tortoise', 'parler' etc.
    """
    backend_params = backend_params or {}
    if backend == 'gTTS':
        print(f"Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)
    elif backend == 'bark' and bark_model is not None and bark_tokenizer is not None:
        print(f"Using Bark for character {character} with emotion '{emotion}': {text[:50]}...")
        # Bark implementation
        # This is a basic implementation; more advanced usage might involve speaker prompts
        try:
            # You can try to inject emotion into the text for Bark
            text_prompt = f"[{emotion}] {text}" if emotion != 'neutral' else text

            inputs = bark_tokenizer(text_prompt, return_tensors="pt").to(device)
            # Generate audio
            with torch.no_grad():
                # Use do_sample=True for more variability, adjust temperature/top_k/top_p if needed
                speech_output = bark_model.generate(**inputs, do_sample=True, temperature=0.7)


            # Convert tensor to numpy array
            audio_array = speech_output[0].cpu().numpy()

            # Save to a temporary WAV file (Bark outputs raw audio)
            tmpfile_wav = f'/content/{uuid.uuid4().hex}.wav'
            sf.write(tmpfile_wav, audio_array, bark_model.generation_config.sample_rate)

            # Convert WAV to MP3 for consistency with the rest of the pipeline
            tmpfile_mp3 = f'/content/{uuid.uuid4().hex}.mp3'
            AudioSegment.from_wav(tmpfile_wav).export(tmpfile_mp3, format="mp3")

            # Clean up the temporary WAV file
            os.remove(tmpfile_wav)

            return tmpfile_mp3
        except Exception as e:
            print(f"Error synthesizing with Bark: {e}")
            # Fallback to gTTS if Bark fails
            print(f"Falling back to gTTS for character {character}.")
            return synthesize_with_gtts(text)

    elif backend == 'tortoise':
        # Tortoise implementation placeholder - will add if installation issues are resolved
        print(f"Tortoise backend selected but not implemented. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(f"[{character} - {emotion}] {text}")
    else:
        print(f"Unknown backend '{backend}'. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)

# Synthesize a scene (all lines) and return combined audio file path (re-include)
def synthesize_scene(scene, char_map, backend='gTTS', music_path=None, intensity=0.3):
    scene_segments = []
    tmp_files = []
    for ln in scene['lines']:
        char = ln['character']
        text = ln['text']
        # Use the emotional cue from the line if available, otherwise fallback to character map or 'neutral'
        emotion = ln.get('emotional_cues', [])
        emotion_str = emotion[0] if emotion else char_map.get(char, {}).get('emotion', 'neutral')

        backend_params = {'voice': char_map.get(char, {}).get('voice')}
        audio_path = synthesize_line_tts_backend(char, text, emotion=emotion_str, backend=backend, backend_params=backend_params)
        tmp_files.append(audio_path)
        seg = AudioSegment.from_file(audio_path)
        scene_segments.append(seg)
    # concatenate
    combined = AudioSegment.silent(duration=0)
    for seg in scene_segments:
        # small pause between lines
        combined += seg + AudioSegment.silent(duration=300)
    # export temp file
    outpath = f'/content/scene_{scene["scene_id"]}_{uuid.uuid4().hex}.mp3'
    combined.export(outpath, format='mp3')

    # Clean up temporary individual line audio files
    for tf in tmp_files:
        try:
            os.remove(tf)
        except OSError as e:
            print(f"Error removing temporary file {tf}: {e}")

    # Mix music if requested
    if music_path:
        mixed = mix_with_background(outpath, music_path=music_path, intensity=intensity)
        # Clean up the combined speech file after mixing
        try:
            os.remove(outpath)
        except OSError as e:
            print(f"Error removing temporary file {outpath}: {e}")
        return mixed
    return outpath


# Synthesize full script (re-include, although not strictly necessary for this test, it's part of the pipeline)
def synthesize_full(scenes, char_map, backend='gTTS', music_path=None, intensity=0.25):
    scene_files = []
    for s in scenes:
        # Pass the backend correctly
        f = synthesize_scene(s, char_map, backend=backend, music_path=music_path, intensity=intensity)
        scene_files.append(f)
    # combine into one large audio
    combined = AudioSegment.silent(duration=0)
    for sfp in scene_files:
        combined += AudioSegment.from_file(sfp) + AudioSegment.silent(duration=500)
    out = f'/content/combined_story_{uuid.uuid4().hex}.mp3'
    combined.export(out, format='mp3')

    # Clean up temporary scene audio files
    for sfp in scene_files:
        try:
            os.remove(sfp)
        except OSError as e:
            print(f"Error removing temporary file {sfp}: {e}")

    return out, scene_files


# Create dummy data for testing synthesis
test_scene = {
    'scene_id': 99,
    'lines': [
        {'character': 'Alice', 'text': "Hello there! I'm feeling quite happy today.", 'emotional_cues': ['happy'], 'sound_effects': []},
        {'character': 'Bob', 'text': "Oh really? I'm a bit worried about the weather.", 'emotional_cues': ['worried'], 'sound_effects': ['rain']},
        {'character': 'Alice', 'text': "Don't worry, I'm sure it will be fine!", 'emotional_cues': [], 'sound_effects': []}
    ],
    'scene_summary': 'Alice and Bob discuss the weather.',
    'emotional_cues': ['happy', 'worried'],
    'sound_effects': ['rain']
}

test_char_map = {
    'Alice': {'voice': 'female_default', 'gender':'female', 'emotion':'neutral'},
    'Bob': {'voice': 'male_default', 'gender':'male', 'emotion':'neutral'}
}

# Synthesize the test scene using the 'bark' backend
print("Synthesizing test scene with Bark...")
try:
    # Ensure the backend parameter is a simple string 'bark'
    synthesized_audio_path = synthesize_scene(test_scene, test_char_map, backend='bark')
    print(f"Synthesis complete. Output file: {synthesized_audio_path}")

    # Optional: Load and check the synthesized audio (requires soundfile and numpy)
    # audio_data, samplerate = sf.read(synthesized_audio_path)
    # print(f"Synthesized audio shape: {audio_data.shape}, Sample rate: {samplerate}")

except Exception as e:
    print(f"An error occurred during Bark synthesis test: {e}")

"""**Reasoning**:
The previous command failed because the `gTTS` module was not found. Although it was listed in the initial installation cell, the current execution environment seems to be missing it. I need to install the `gTTS` library to resolve this import error and allow the fallback mechanism in `synthesize_line_tts_backend` and `synthesize_with_gtts` to function. After installation, I will re-run the code that defines the synthesis functions and tests the Bark implementation.


"""

# Install gTTS
!pip install --quiet gTTS

# Re-include necessary imports and function definitions
import torch
from transformers import BarkModel, AutoTokenizer
from pydub import AudioSegment
import soundfile as sf
import numpy as np
import os
import uuid

# gTTS is now imported from the installation step

# Check for GPU and set device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load Bark model and tokenizer (re-loading in case the kernel state was reset)
try:
    bark_tokenizer = AutoTokenizer.from_pretrained("suno/bark")
    bark_model = BarkModel.from_pretrained("suno/bark").to(device)
    print("Bark model and tokenizer loaded successfully.")
except Exception as e:
    print(f"Error loading Bark model: {e}")
    bark_tokenizer = None
    bark_model = None

# Fallback TTS using gTTS (re-include as it's a dependency)
def synthesize_with_gtts(text, lang='en'):
    tts = gTTS(text=text, lang=lang)
    tmpfile = f'/content/{uuid.uuid4().hex}.mp3'
    tts.save(tmpfile)
    return tmpfile

# Mix background music with ducking (re-include as it's a dependency)
def mix_with_background(speech_path, music_path=None, intensity=0.3):
    speech = AudioSegment.from_file(speech_path)
    if not music_path:
        return speech
    music = AudioSegment.from_file(music_path)
    # make music match duration
    if len(music) < len(speech):
        # loop music
        times = int(len(speech) / len(music)) + 1
        music = music * times
    music = music[:len(speech)]
    # duck music
    music = music - (10 * (1 - intensity))  # simple attenuation
    mixed = speech.overlay(music)
    outpath = f'/content/{uuid.uuid4().hex}_mixed.mp3'
    mixed.export(outpath, format='mp3')
    return outpath


# Update the synthesize_line_tts_backend function (re-include)
def synthesize_line_tts_backend(character, text, emotion='neutral', backend='gTTS', backend_params=None):
    """Synthesize a single line to an audio file path.
    backend: 'gTTS', 'bark', 'tortoise', 'parler' etc.
    """
    backend_params = backend_params or {}
    if backend == 'gTTS':
        print(f"Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)
    elif backend == 'bark' and bark_model is not None and bark_tokenizer is not None:
        print(f"Using Bark for character {character} with emotion '{emotion}': {text[:50]}...")
        # Bark implementation
        # This is a basic implementation; more advanced usage might involve speaker prompts
        try:
            # You can try to inject emotion into the text for Bark
            text_prompt = f"[{emotion}] {text}" if emotion != 'neutral' else text

            inputs = bark_tokenizer(text_prompt, return_tensors="pt").to(device)
            # Generate audio
            with torch.no_grad():
                # Use do_sample=True for more variability, adjust temperature/top_k/top_p if needed
                speech_output = bark_model.generate(**inputs, do_sample=True, temperature=0.7)


            # Convert tensor to numpy array
            audio_array = speech_output[0].cpu().numpy()

            # Save to a temporary WAV file (Bark outputs raw audio)
            tmpfile_wav = f'/content/{uuid.uuid4().hex}.wav'
            sf.write(tmpfile_wav, audio_array, bark_model.generation_config.sample_rate)

            # Convert WAV to MP3 for consistency with the rest of the pipeline
            tmpfile_mp3 = f'/content/{uuid.uuid4().hex}.mp3'
            AudioSegment.from_wav(tmpfile_wav).export(tmpfile_mp3, format="mp3")

            # Clean up the temporary WAV file
            os.remove(tmpfile_wav)

            return tmpfile_mp3
        except Exception as e:
            print(f"Error synthesizing with Bark: {e}")
            # Fallback to gTTS if Bark fails
            print(f"Falling back to gTTS for character {character}.")
            return synthesize_with_gtts(text)

    elif backend == 'tortoise':
        # Tortoise implementation placeholder - will add if installation issues are resolved
        print(f"Tortoise backend selected but not implemented. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(f"[{character} - {emotion}] {text}")
    else:
        print(f"Unknown backend '{backend}'. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)

# Synthesize a scene (all lines) and return combined audio file path (re-include)
def synthesize_scene(scene, char_map, backend='gTTS', music_path=None, intensity=0.3):
    scene_segments = []
    tmp_files = []
    for ln in scene['lines']:
        char = ln['character']
        text = ln['text']
        # Use the emotional cue from the line if available, otherwise fallback to character map or 'neutral'
        emotion = ln.get('emotional_cues', [])
        emotion_str = emotion[0] if emotion else char_map.get(char, {}).get('emotion', 'neutral')

        backend_params = {'voice': char_map.get(char, {}).get('voice')}
        audio_path = synthesize_line_tts_backend(char, text, emotion=emotion_str, backend=backend, backend_params=backend_params)
        tmp_files.append(audio_path)
        seg = AudioSegment.from_file(audio_path)
        scene_segments.append(seg)
    # concatenate
    combined = AudioSegment.silent(duration=0)
    for seg in scene_segments:
        # small pause between lines
        combined += seg + AudioSegment.silent(duration=300)
    # export temp file
    outpath = f'/content/scene_{scene["scene_id"]}_{uuid.uuid4().hex}.mp3'
    combined.export(outpath, format='mp3')

    # Clean up temporary individual line audio files
    for tf in tmp_files:
        try:
            os.remove(tf)
        except OSError as e:
            print(f"Error removing temporary file {tf}: {e}")

    # Mix music if requested
    if music_path:
        mixed = mix_with_background(outpath, music_path=music_path, intensity=intensity)
        # Clean up the combined speech file after mixing
        try:
            os.remove(outpath)
        except OSError as e:
            print(f"Error removing temporary file {outpath}: {e}")
        return mixed
    return outpath


# Synthesize full script (re-include, although not strictly necessary for this test, it's part of the pipeline)
def synthesize_full(scenes, char_map, backend='gTTS', music_path=None, intensity=0.25):
    scene_files = []
    for s in scenes:
        # Pass the backend correctly
        f = synthesize_scene(s, char_map, backend=backend, music_path=music_path, intensity=intensity)
        scene_files.append(f)
    # combine into one large audio
    combined = AudioSegment.silent(duration=0)
    for sfp in scene_files:
        combined += AudioSegment.from_file(sfp) + AudioSegment.silent(duration=500)
    out = f'/content/combined_story_{uuid.uuid4().hex}.mp3'
    combined.export(out, format='mp3')

    # Clean up temporary scene audio files
    for sfp in scene_files:
        try:
            os.remove(sfp)
        except OSError as e:
            print(f"Error removing temporary file {sfp}: {e}")

    return out, scene_files


# Create dummy data for testing synthesis (re-create as kernel state might be an issue)
test_scene = {
    'scene_id': 99,
    'lines': [
        {'character': 'Alice', 'text': "Hello there! I'm feeling quite happy today.", 'emotional_cues': ['happy'], 'sound_effects': []},
        {'character': 'Bob', 'text': "Oh really? I'm a bit worried about the weather.", 'emotional_cues': ['worried'], 'sound_effects': ['rain']},
        {'character': 'Alice', 'text': "Don't worry, I'm sure it will be fine!", 'emotional_cues': [], 'sound_effects': []}
    ],
    'scene_summary': 'Alice and Bob discuss the weather.',
    'emotional_cues': ['happy', 'worried'],
    'sound_effects': ['rain']
}

test_char_map = {
    'Alice': {'voice': 'female_default', 'gender':'female', 'emotion':'neutral'},
    'Bob': {'voice': 'male_default', 'gender':'male', 'emotion':'neutral'}
}

# Synthesize the test scene using the 'bark' backend
print("Synthesizing test scene with Bark...")
try:
    # Ensure the backend parameter is a simple string 'bark'
    synthesized_audio_path = synthesize_scene(test_scene, test_char_map, backend='bark')
    print(f"Synthesis complete. Output file: {synthesized_audio_path}")

    # Optional: Load and check the synthesized audio (requires soundfile and numpy)
    # audio_data, samplerate = sf.read(synthesized_audio_path)
    # print(f"Synthesized audio shape: {audio_data.shape}, Sample rate: {samplerate}")

except Exception as e:
    print(f"An error occurred during Bark synthesis test: {e}")

"""## Integrate advanced tts models

### Subtask:
Retry integrating the Bark TTS model, focusing on resolving the tensor size mismatch error during synthesis and ensuring the gTTS fallback is functional.

**Reasoning**:
Re-import necessary libraries, ensure the Bark model and tokenizer are loaded correctly, and refine the Bark synthesis logic to address the tensor size mismatch error, while also ensuring the gTTS fallback is correctly implemented and accessible.
"""

import torch
from transformers import BarkModel, AutoTokenizer
from pydub import AudioSegment
import soundfile as sf
import numpy as np
import os
import uuid
from gtts import gTTS # Ensure gTTS is imported here

# Check for GPU and set device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load Bark model and tokenizer
# Using a try-except block to handle potential loading errors
try:
    # Explicitly specify device when loading
    bark_tokenizer = AutoTokenizer.from_pretrained("suno/bark")
    bark_model = BarkModel.from_pretrained("suno/bark").to(device)
    print("Bark model and tokenizer loaded successfully.")
except Exception as e:
    print(f"Error loading Bark model: {e}")
    bark_tokenizer = None
    bark_model = None
    print("Bark model loading failed. Bark synthesis will not be available.")


# Fallback TTS using gTTS
def synthesize_with_gtts(text, lang='en'):
    """Synthesize text using Google Text-to-Speech."""
    try:
        tts = gTTS(text=text, lang=lang)
        tmpfile = f'/content/{uuid.uuid4().hex}.mp3'
        tts.save(tmpfile)
        return tmpfile
    except Exception as e:
        print(f"Error during gTTS synthesis: {e}")
        return None # Return None if gTTS fails


# Mix background music with ducking (re-include for completeness)
def mix_with_background(speech_path, music_path=None, intensity=0.3):
    """Mix speech audio with background music."""
    try:
        speech = AudioSegment.from_file(speech_path)
        if not music_path or not os.path.exists(music_path):
            return speech # Return speech only if no music or music file not found

        music = AudioSegment.from_file(music_path)
        # make music match duration
        if len(music) < len(speech):
            # loop music
            times = int(len(speech) / len(music)) + 1
            music = music * times
        music = music[:len(speech)]
        # duck music
        music = music - (10 * (1 - intensity))  # simple attenuation
        mixed = speech.overlay(music)
        outpath = f'/content/{uuid.uuid4().hex}_mixed.mp3'
        mixed.export(outpath, format='mp3')
        return outpath
    except Exception as e:
        print(f"Error mixing audio with background: {e}")
        return speech_path # Return original speech path on error


# Update the synthesize_line_tts_backend function
def synthesize_line_tts_backend(character, text, emotion='neutral', backend='gTTS', backend_params=None):
    """Synthesize a single line to an audio file path using chosen backend."""
    backend_params = backend_params or {}

    if backend == 'bark' and bark_model is not None and bark_tokenizer is not None:
        print(f"Attempting Bark synthesis for character {character} with emotion '{emotion}': {text[:50]}...")
        try:
            # Simplify input for initial testing to isolate tensor issue
            # Remove emotional cue injection for now
            # text_prompt = f"[{emotion}] {text}" if emotion != 'neutral' else text
            text_prompt = text # Use raw text

            # Tokenize text
            # Ensure the inputs tensor is on the correct device
            inputs = bark_tokenizer(text_prompt, return_tensors="pt").to(device)

            # Check input tensor shape before generation (for debugging)
            # print(f"Bark input tensor shape: {inputs['input_ids'].shape}")

            # Generate audio
            with torch.no_grad():
                # Use do_sample=True for more variability
                # Add output_attentions=False, output_hidden_states=False to simplify output if needed
                # Check model's expected input format if errors persist
                speech_output = bark_model.generate(**inputs, do_sample=True, temperature=0.7,
                                                    output_attentions=False, output_hidden_states=False)


            # Convert tensor to numpy array
            audio_array = speech_output[0].cpu().numpy()

            # Save to a temporary WAV file (Bark outputs raw audio)
            tmpfile_wav = f'/content/{uuid.uuid4().hex}.wav'
            sf.write(tmpfile_wav, audio_array, bark_model.generation_config.sample_rate)

            # Convert WAV to MP3
            tmpfile_mp3 = f'/content/{uuid.uuid4().hex}.mp3'
            AudioSegment.from_wav(tmpfile_wav).export(tmpfile_mp3, format="mp3")

            # Clean up the temporary WAV file
            os.remove(tmpfile_wav)

            print(f"Bark synthesis successful. Output: {tmpfile_mp3}")
            return tmpfile_mp3
        except Exception as e:
            print(f"Error synthesizing with Bark: {e}")
            # Fallback to gTTS if Bark fails
            print(f"Falling back to gTTS for character {character}.")
            return synthesize_with_gtts(text) # Call gTTS fallback

    elif backend == 'gTTS':
        print(f"Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text) # Call gTTS directly

    elif backend == 'tortoise':
        # Tortoise implementation placeholder - will add if installation issues are resolved
        print(f"Tortoise backend selected but not implemented. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(f"[{character} - {emotion}] {text}")
    else:
        print(f"Unknown backend '{backend}'. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text) # Default to gTTS


# Synthesize a scene (all lines) and return combined audio file path (re-include)
def synthesize_scene(scene, char_map, backend='gTTS', music_path=None, intensity=0.3):
    """Synthesize all lines in a scene and combine them."""
    scene_segments = []
    tmp_files = []
    for ln in scene['lines']:
        char = ln['character']
        text = ln['text']
        # Use the emotional cue from the line if available, otherwise fallback to character map or 'neutral'
        emotion = ln.get('emotional_cues', [])
        emotion_str = emotion[0] if emotion else char_map.get(char, {}).get('emotion', 'neutral')

        backend_params = {'voice': char_map.get(char, {}).get('voice')}
        # Call the updated synthesize_line_tts_backend
        audio_path = synthesize_line_tts_backend(char, text, emotion=emotion_str, backend=backend, backend_params=backend_params)

        if audio_path and os.path.exists(audio_path): # Only process if synthesis was successful
            tmp_files.append(audio_path)
            try:
                seg = AudioSegment.from_file(audio_path)
                scene_segments.append(seg)
            except Exception as e:
                 print(f"Error loading audio segment from {audio_path}: {e}")
                 # Decide whether to skip this segment or handle differently
                 pass # Skipping the problematic segment for now

    if not scene_segments:
        print(f"No audio segments generated for scene {scene['scene_id']}.")
        return None # Return None if no audio was generated

    # concatenate
    combined = AudioSegment.silent(duration=0)
    for i, seg in enumerate(scene_segments):
        combined += seg
        if i < len(scene_segments) - 1: # Add pause only between segments
            combined += AudioSegment.silent(duration=300) # Small pause between lines

    # export temp file
    outpath = f'/content/scene_{scene["scene_id"]}_{uuid.uuid4().hex}.mp3'
    try:
        combined.export(outpath, format='mp3')
    except Exception as e:
        print(f"Error exporting combined scene audio to {outpath}: {e}")
        outpath = None # Indicate export failed

    # Clean up temporary individual line audio files
    for tf in tmp_files:
        try:
            os.remove(tf)
        except OSError as e:
            print(f"Error removing temporary file {tf}: {e}")

    # Mix music if requested and if combined audio was exported
    if music_path and outpath and os.path.exists(music_path):
        mixed = mix_with_background(outpath, music_path=music_path, intensity=intensity)
        # Clean up the combined speech file after mixing
        try:
            os.remove(outpath)
        except OSError as e:
            print(f"Error removing temporary file {outpath}: {e}")
        return mixed
    elif outpath:
         return outpath # Return combined speech path if no music or mixing failed
    else:
         return None # Return None if combined audio export failed


# Synthesize full script (re-include, although not strictly necessary for this test, it's part of the pipeline)
def synthesize_full(scenes, char_map, backend='gTTS', music_path=None, intensity=0.25):
    """Synthesize all scenes and combine into a full story audio."""
    scene_files = []
    for s in scenes:
        # Pass the backend correctly
        f = synthesize_scene(s, char_map, backend=backend, music_path=music_path, intensity=intensity)
        if f and os.path.exists(f): # Only add if scene synthesis was successful
            scene_files.append(f)

    if not scene_files:
        print("No scene audio files generated for the full script.")
        return None, [] # Return None and empty list if no scenes were synthesized

    # combine into one large audio
    combined = AudioSegment.silent(duration=0)
    for i, sfp in enumerate(scene_files):
        try:
            combined += AudioSegment.from_file(sfp)
            if i < len(scene_files) - 1: # Add pause only between scenes
                 combined += AudioSegment.silent(duration=500) # Pause between scenes
        except Exception as e:
             print(f"Error loading scene audio file {sfp}: {e}")
             pass # Skip problematic scene file

    if combined.duration_seconds == 0:
         print("Combined audio is empty.")
         return None, scene_files # Return None if combined audio is empty

    out = f'/content/combined_story_{uuid.uuid4().hex}.mp3'
    try:
        combined.export(out, format='mp3')
    except Exception as e:
        print(f"Error exporting combined story audio to {out}: {e}")
        out = None # Indicate export failed


    # Clean up temporary scene audio files
    for sfp in scene_files:
        try:
            os.remove(sfp)
        except OSError as e:
            print(f"Error removing temporary file {sfp}: {e}")

    return out, scene_files


# Create dummy data for testing synthesis (re-create to be safe)
test_scene = {
    'scene_id': 99,
    'lines': [
        {'character': 'Alice', 'text': "Hello there! I'm feeling quite happy today.", 'emotional_cues': ['happy'], 'sound_effects': []},
        {'character': 'Bob', 'text': "Oh really? I'm a bit worried about the weather.", 'emotional_cues': ['worried'], 'sound_effects': ['rain']},
        {'character': 'Alice', 'text': "Don't worry, I'm sure it will be fine!", 'emotional_cues': [], 'sound_effects': []}
    ],
    'scene_summary': 'Alice and Bob discuss the weather.',
    'emotional_cues': ['happy', 'worried'],
    'sound_effects': ['rain']
}

test_char_map = {
    'Alice': {'voice': 'female_default', 'gender':'female', 'emotion':'neutral'},
    'Bob': {'voice': 'male_default', 'gender':'male', 'emotion':'neutral'}
}

# Synthesize the test scene using the 'bark' backend
print("\n--- Synthesizing test scene with Bark ---")
try:
    # Ensure the backend parameter is a simple string 'bark'
    synthesized_audio_path = synthesize_scene(test_scene, test_char_map, backend='bark')

    if synthesized_audio_path and os.path.exists(synthesized_audio_path):
        print(f"Synthesis complete. Output file: {synthesized_audio_path}")
        # Optional: Load and check the synthesized audio (requires soundfile and numpy)
        # audio_data, samplerate = sf.read(synthesized_audio_path)
        # print(f"Synthesized audio shape: {audio_data.shape}, Sample rate: {samplerate}")
    elif synthesized_audio_path is None:
         print("Synthesis failed or returned None.")
    else:
         print(f"Synthesis complete, but output file {synthesized_audio_path} does not exist.")


except Exception as e:
    print(f"An error occurred during Bark synthesis test: {e}")

print("\n--- Synthesizing test scene with gTTS fallback ---")
# Test gTTS fallback explicitly
try:
    synthesized_audio_path_gtts = synthesize_scene(test_scene, test_char_map, backend='gTTS')
    if synthesized_audio_path_gtts and os.path.exists(synthesized_audio_path_gtts):
        print(f"gTTS fallback synthesis complete. Output file: {synthesized_audio_path_gtts}")
    elif synthesized_audio_path_gtts is None:
         print("gTTS fallback synthesis failed or returned None.")
    else:
         print(f"gTTS fallback synthesis complete, but output file {synthesized_audio_path_gtts} does not exist.")
except Exception as e:
     print(f"An error occurred during gTTS fallback test: {e}")

"""## Implement character-to-voice mapping

### Subtask:
Develop a more robust system for mapping detected characters to specific voice presets within the chosen TTS model.

**Reasoning**:
Update the `map_characters_to_voices` function to include more diverse default voice options or placeholders for model-specific voice IDs, and modify the `synthesize_line_tts_backend` function to accept and utilize these voice parameters when the chosen backend supports them. Also, update the `char_map` structure and consider fallback mechanisms.
"""

# Update map_characters_to_voices to include more options/placeholders
def map_characters_to_voices(characters):
    default_voices = {}
    # Define diverse default voice options or placeholders for Bark/Tortoise/etc.
    # These are placeholders; replace with actual model voice names/IDs
    voice_options = {
        'female': ['v2/en_speaker_6', 'v2/en_speaker_7', 'female_default_gtts'],
        'male': ['v2/en_speaker_9', 'v2/en_speaker_10', 'male_default_gtts'],
        'neutral': ['v2/en_speaker_0', 'neutral_default_gtts'] # More generic or child voices could go here
    }

    male_idx = 0
    female_idx = 0
    neutral_idx = 0

    for i, c in enumerate(characters):
        # Simple heuristic for gender based on common names (very basic)
        # A more robust system might use a name-gender library or user input
        lower_c = c.lower()
        gender = 'neutral' # Default to neutral

        if any(male_name.lower() in lower_c for male_name in ['john', 'bob', 'david', 'mark', 'arun', 'mr', 'sir']):
            gender = 'male'
        elif any(female_name.lower() in lower_c for female_name in ['alice', 'mary', 'sophia', 'nisha', 'leela', 'ms', 'mrs', 'miss']):
            gender = 'female'

        selected_voice = None
        if gender == 'male' and male_idx < len(voice_options['male']):
            selected_voice = voice_options['male'][male_idx]
            male_idx += 1
        elif gender == 'female' and female_idx < len(voice_options['female']):
            selected_voice = voice_options['female'][female_idx]
            female_idx += 1
        elif neutral_idx < len(voice_options['neutral']):
             selected_voice = voice_options['neutral'][neutral_idx]
             neutral_idx += 1
        else:
            # Cycle through voices if more characters than options
            all_voices = voice_options['female'] + voice_options['male'] + voice_options['neutral']
            selected_voice = all_voices[i % len(all_voices)]
            if 'female' in selected_voice: gender = 'female'
            elif 'male' in selected_voice: gender = 'male'
            else: gender = 'neutral'


        default_voices[c] = {'voice': selected_voice, 'gender': gender, 'emotion': 'neutral'}

    return default_voices

# Modify synthesize_line_tts_backend to utilize voice parameters
def synthesize_line_tts_backend(character, text, emotion='neutral', backend='gTTS', backend_params=None):
    """Synthesize a single line to an audio file path using chosen backend."""
    backend_params = backend_params or {}
    selected_voice = backend_params.get('voice') # Get the selected voice from char_map

    if backend == 'bark' and bark_model is not None and bark_tokenizer is not None:
        print(f"Attempting Bark synthesis for character {character} with voice '{selected_voice}' and emotion '{emotion}': {text[:50]}...")
        try:
            # Bark can use speaker prompts. If selected_voice is a Bark speaker ID:
            if selected_voice and selected_voice.startswith('v2/'):
                 # For Bark, voice is typically handled by the history prompt (e.g., a few seconds of target voice audio)
                 # or by using built-in speaker presets like 'v2/en_speaker_6'.
                 # We'll try using the speaker preset ID if provided.
                 # The prompt format for speaker presets is typically "[voice_id] text"
                 text_prompt = f"[{selected_voice}] {text}"
            else:
                 # If no specific Bark voice ID is given, use the text directly
                 text_prompt = text

            # Tokenize text
            inputs = bark_tokenizer(text_prompt, return_tensors="pt").to(device)

            # Generate audio
            with torch.no_grad():
                # Adjust parameters as needed for expressiveness
                speech_output = bark_model.generate(**inputs, do_sample=True, temperature=0.7,
                                                    output_attentions=False, output_hidden_states=False)

            # Convert tensor to numpy array
            audio_array = speech_output[0].cpu().numpy()

            # Save to a temporary WAV file (Bark outputs raw audio)
            tmpfile_wav = f'/content/{uuid.uuid4().hex}.wav'
            sf.write(tmpfile_wav, audio_array, bark_model.generation_config.sample_rate)

            # Convert WAV to MP3
            tmpfile_mp3 = f'/content/{uuid.uuid4().hex}.mp3'
            AudioSegment.from_wav(tmpfile_wav).export(tmpfile_mp3, format="mp3")

            # Clean up the temporary WAV file
            os.remove(tmpfile_wav)

            print(f"Bark synthesis successful. Output: {tmpfile_mp3}")
            return tmpfile_mp3
        except Exception as e:
            print(f"Error synthesizing with Bark: {e}")
            # Fallback to gTTS if Bark fails
            print(f"Falling back to gTTS for character {character}.")
            # For gTTS fallback, the selected_voice parameter isn't used directly by gTTS,
            # but the text is synthesized.
            return synthesize_with_gtts(text)

    elif backend == 'gTTS':
        print(f"Using gTTS for character {character}: {text[:50]}...")
        # gTTS does not support multiple voices or voice IDs in the same way
        # we just synthesize the text.
        return synthesize_with_gtts(text)

    elif backend == 'tortoise':
        # Tortoise implementation placeholder - will add if installation issues are resolved
        print(f"Tortoise backend selected but not implemented. Using gTTS for character {character}: {text[:50]}...")
        # Placeholder: Ideally, Tortoise would use the selected_voice parameter
        # as a speaker ID or reference audio path.
        return synthesize_with_gtts(f"[{character} - {emotion}] {text}")
    else:
        print(f"Unknown backend '{backend}'. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)


# Update process_pdf_and_prepare_script to use the updated map_characters_to_voices
def process_pdf_and_prepare_script(uploaded_file_path):
    """Process uploaded PDF to extract text, detect characters/scenes, and map voices."""
    full_text, pages = extract_text_from_pdf_fp(uploaded_file_path)
    characters, scenes = detect_characters_and_dialogue(full_text)
    # Generate simple scene summaries (first 2 sentences)
    for s in scenes:
        txt = ' '.join([ln['text'] for ln in s['lines']])
        s['scene_summary'] = ' '.join(txt.split('. ')[:2]) # Re-join the sentences for summary
    char_map = map_characters_to_voices(characters)
    return {'full_text': full_text, 'pages': pages, 'characters': characters, 'scenes': scenes, 'char_map': char_map}

# Re-run the test with the updated functions
print("\n--- Testing updated map_characters_to_voices and synthesize_line_tts_backend ---")

# Create dummy data for testing
test_characters = ['Alice', 'Bob', 'Narrator', 'Child']
test_char_map_updated = map_characters_to_voices(test_characters)
print("Updated char_map:")
print(test_char_map_updated)

test_scene_updated = {
    'scene_id': 100,
    'lines': [
        {'character': 'Alice', 'text': "This is Alice speaking.", 'emotional_cues': [], 'sound_effects': []},
        {'character': 'Bob', 'text': "And this is Bob's line.", 'emotional_cues': [], 'sound_effects': []},
        {'character': 'Narrator', 'text': "Meanwhile, the story continued.", 'emotional_cues': [], 'sound_effects': []},
        {'character': 'Child', 'text': "Can I have some candy?", 'emotional_cues': [], 'sound_effects': []}
    ],
    'scene_summary': 'Testing different voices',
    'emotional_cues': [],
    'sound_effects': []
}

# Test synthesis with the updated char_map and Bark backend
print("\n--- Synthesizing test scene with Bark and updated voices ---")
try:
    # Pass the updated char_map
    synthesized_audio_path_updated_bark = synthesize_scene(test_scene_updated, test_char_map_updated, backend='bark')

    if synthesized_audio_path_updated_bark and os.path.exists(synthesized_audio_path_updated_bark):
        print(f"Updated Bark synthesis complete. Output file: {synthesized_audio_path_updated_bark}")
    elif synthesized_audio_path_updated_bark is None:
         print("Updated Bark synthesis failed or returned None.")
    else:
         print(f"Updated Bark synthesis complete, but output file {synthesized_audio_path_updated_bark} does not exist.")


except Exception as e:
    print(f"An error occurred during updated Bark synthesis test: {e}")

print("\n--- Synthesizing test scene with gTTS and updated voices ---")
# Test synthesis with the updated char_map and gTTS backend
try:
    # Pass the updated char_map
    synthesized_audio_path_updated_gtts = synthesize_scene(test_scene_updated, test_char_map_updated, backend='gTTS')

    if synthesized_audio_path_updated_gtts and os.path.exists(synthesized_audio_path_updated_gtts):
        print(f"Updated gTTS synthesis complete. Output file: {synthesized_audio_path_updated_gtts}")
    elif synthesized_audio_path_updated_gtts is None:
         print("Updated gTTS synthesis failed or returned None.")
    else:
         print(f"Updated gTTS synthesis complete, but output file {synthesized_audio_path_updated_gtts} does not exist.")

except Exception as e:
    print(f"An error occurred during updated gTTS synthesis test: {e}")

"""## Enhance emotional synthesis

### Subtask:
Utilize the capabilities of the chosen TTS models to synthesize speech with appropriate emotions based on the detected emotional cues in the text.

**Reasoning**:
Modify the synthesis function to incorporate emotional cues into the prompt for Bark, while keeping the gTTS fallback as is. Update the synthesize_scene function to pass the emotional cues. Test with a scene containing cues.
"""

import torch
from transformers import BarkModel, AutoTokenizer
from pydub import AudioSegment
import soundfile as sf
import numpy as np
import os
import uuid
from gtts import gTTS # Ensure gTTS is imported here

# Check for GPU and set device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load Bark model and tokenizer
# Using a try-except block to handle potential loading errors
try:
    # Explicitly specify device when loading
    bark_tokenizer = AutoTokenizer.from_pretrained("suno/bark")
    bark_model = BarkModel.from_pretrained("suno/bark").to(device)
    print("Bark model and tokenizer loaded successfully.")
except Exception as e:
    print(f"Error loading Bark model: {e}")
    bark_tokenizer = None
    bark_model = None
    print("Bark model loading failed. Bark synthesis will not be available.")


# Fallback TTS using gTTS
def synthesize_with_gtts(text, lang='en'):
    """Synthesize text using Google Text-to-Speech."""
    try:
        tts = gTTS(text=text, lang=lang)
        tmpfile = f'/content/{uuid.uuid4().hex}.mp3'
        tts.save(tmpfile)
        return tmpfile
    except Exception as e:
        print(f"Error during gTTS synthesis: {e}")
        return None # Return None if gTTS fails


# Mix background music with ducking (re-include for completeness)
def mix_with_background(speech_path, music_path=None, intensity=0.3):
    """Mix speech audio with background music."""
    try:
        speech = AudioSegment.from_file(speech_path)
        if not music_path or not os.path.exists(music_path):
            return speech # Return speech only if no music or music file not found

        music = AudioSegment.from_file(music_path)
        # make music match duration
        if len(music) < len(speech):
            # loop music
            times = int(len(speech) / len(music)) + 1
            music = music * times
        music = music[:len(speech)]
        # duck music
        music = music - (10 * (1 - intensity))  # simple attenuation
        mixed = speech.overlay(music)
        outpath = f'/content/{uuid.uuid4().hex}_mixed.mp3'
        mixed.export(outpath, format='mp3')
        return outpath
    except Exception as e:
        print(f"Error mixing audio with background: {e}")
        return speech_path # Return original speech path on error


# Update the synthesize_line_tts_backend function to incorporate emotional cues
def synthesize_line_tts_backend(character, text, emotion='neutral', backend='gTTS', backend_params=None):
    """Synthesize a single line to an audio file path using chosen backend."""
    backend_params = backend_params or {}
    selected_voice = backend_params.get('voice') # Get the selected voice from char_map

    if backend == 'bark' and bark_model is not None and bark_tokenizer is not None:
        print(f"Attempting Bark synthesis for character {character} with voice '{selected_voice}' and emotion '{emotion}': {text[:50]}...")
        try:
            # Incorporate emotional cue into the text prompt for Bark
            # Experiment with format: "[emotion] text"
            if emotion and emotion != 'neutral':
                # Simple format for adding emotion to prompt
                text_prompt = f"[{emotion}] {text}"
            elif selected_voice and selected_voice.startswith('v2/'):
                 # If no emotion but a Bark voice ID, use the speaker preset format
                 text_prompt = f"[{selected_voice}] {text}"
            else:
                 # Default to raw text if no emotion or specific Bark voice ID
                 text_prompt = text


            # Tokenize text
            # Ensure the inputs tensor is on the correct device
            inputs = bark_tokenizer(text_prompt, return_tensors="pt").to(device)

            # Generate audio
            with torch.no_grad():
                # Use do_sample=True for more variability
                speech_output = bark_model.generate(**inputs, do_sample=True, temperature=0.7,
                                                    output_attentions=False, output_hidden_states=False)


            # Convert tensor to numpy array
            audio_array = speech_output[0].cpu().numpy()

            # Save to a temporary WAV file (Bark outputs raw audio)
            tmpfile_wav = f'/content/{uuid.uuid4().hex}.wav'
            sf.write(tmpfile_wav, audio_array, bark_model.generation_config.sample_rate)

            # Convert WAV to MP3
            tmpfile_mp3 = f'/content/{uuid.uuid4().hex}.mp3'
            AudioSegment.from_wav(tmpfile_wav).export(tmpfile_mp3, format="mp3")

            # Clean up the temporary WAV file
            os.remove(tmpfile_wav)

            print(f"Bark synthesis successful. Output: {tmpfile_mp3}")
            return tmpfile_mp3
        except Exception as e:
            print(f"Error synthesizing with Bark: {e}")
            # Fallback to gTTS if Bark fails
            print(f"Falling back to gTTS for character {character}.")
            # For gTTS fallback, the selected_voice and emotion parameters aren't used directly by gTTS,
            # but the text is synthesized.
            return synthesize_with_gtts(text)

    elif backend == 'gTTS':
        print(f"Using gTTS for character {character}: {text[:50]}...")
        # gTTS does not support explicit emotional control or voice IDs in the same way
        # we just synthesize the text. The emotional content might be inferred from the text itself.
        return synthesize_with_gtts(text)

    elif backend == 'tortoise':
        # Tortoise implementation placeholder
        print(f"Tortoise backend selected but not implemented. Using gTTS for character {character}: {text[:50]}...")
        # Placeholder: Ideally, Tortoise would use the selected_voice parameter
        # as a speaker ID or reference audio path and potentially emotion prompts.
        return synthesize_with_gtts(f"[{character} - {emotion}] {text}")
    else:
        print(f"Unknown backend '{backend}'. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)


# Synthesize a scene (all lines) and return combined audio file path (re-include)
def synthesize_scene(scene, char_map, backend='gTTS', music_path=None, intensity=0.3):
    """Synthesize all lines in a scene and combine them."""
    scene_segments = []
    tmp_files = []
    for ln in scene['lines']:
        char = ln['character']
        text = ln['text']
        # Use the first emotional cue from the line if available, otherwise fallback to character map or 'neutral'
        # Pass the list of emotional cues found in the line
        emotion = ln.get('emotional_cues', [])
        # If multiple emotions are detected, you might choose the first one, or handle them differently
        emotion_str = emotion[0] if emotion else char_map.get(char, {}).get('emotion', 'neutral')


        backend_params = {'voice': char_map.get(char, {}).get('voice')}
        # Call the updated synthesize_line_tts_backend, passing the determined emotion_str
        audio_path = synthesize_line_tts_backend(char, text, emotion=emotion_str, backend=backend, backend_params=backend_params)

        if audio_path and os.path.exists(audio_path): # Only process if synthesis was successful
            tmp_files.append(audio_path)
            try:
                seg = AudioSegment.from_file(audio_path)
                scene_segments.append(seg)
            except Exception as e:
                 print(f"Error loading audio segment from {audio_path}: {e}")
                 pass # Skipping the problematic segment for now

    if not scene_segments:
        print(f"No audio segments generated for scene {scene['scene_id']}.")
        return None # Return None if no audio was generated

    # concatenate
    combined = AudioSegment.silent(duration=0)
    for i, seg in enumerate(scene_segments):
        combined += seg
        if i < len(scene_segments) - 1: # Add pause only between segments
            combined += AudioSegment.silent(duration=300) # Small pause between lines

    # export temp file
    outpath = f'/content/scene_{scene["scene_id"]}_{uuid.uuid4().hex}.mp3'
    try:
        combined.export(outpath, format='mp3')
    except Exception as e:
        print(f"Error exporting combined scene audio to {outpath}: {e}")
        outpath = None # Indicate export failed

    # Clean up temporary individual line audio files
    for tf in tmp_files:
        try:
            os.remove(tf)
        except OSError as e:
            print(f"Error removing temporary file {tf}: {e}")

    # Mix music if requested and if combined audio was exported
    if music_path and outpath and os.path.exists(music_path):
        mixed = mix_with_background(outpath, music_path=music_path, intensity=intensity)
        # Clean up the combined speech file after mixing
        try:
            os.remove(outpath)
        except OSError as e:
            print(f"Error removing temporary file {outpath}: {e}")
        return mixed
    elif outpath:
         return outpath # Return combined speech path if no music or mixing failed
    else:
         return None # Return None if combined audio export failed


# Synthesize full script (re-include for completeness)
def synthesize_full(scenes, char_map, backend='gTTS', music_path=None, intensity=0.25):
    """Synthesize all scenes and combine into a full story audio."""
    scene_files = []
    for s in scenes:
        # Pass the backend correctly
        f = synthesize_scene(s, char_map, backend=backend, music_path=music_path, intensity=intensity)
        if f and os.path.exists(f): # Only add if scene synthesis was successful
            scene_files.append(f)

    if not scene_files:
        print("No scene audio files generated for the full script.")
        return None, [] # Return None and empty list if no scenes were synthesized

    # combine into one large audio
    combined = AudioSegment.silent(duration=0)
    for i, sfp in enumerate(scene_files):
        try:
            combined += AudioSegment.from_file(sfp)
            if i < len(scene_files) - 1: # Add pause only between scenes
                 combined += AudioSegment.silent(duration=500) # Pause between scenes
        except Exception as e:
             print(f"Error loading scene audio file {sfp}: {e}")
             pass # Skip problematic scene file

    if combined.duration_seconds == 0:
         print("Combined audio is empty.")
         return None, scene_files # Return None if combined audio is empty


    out = f'/content/combined_story_{uuid.uuid4().hex}.mp3'
    try:
        combined.export(out, format='mp3')
    except Exception as e:
        print(f"Error exporting combined story audio to {out}: {e}")
        out = None # Indicate export failed


    # Clean up temporary scene audio files
    for sfp in scene_files:
        try:
            os.remove(sfp)
        except OSError as e:
            print(f"Error removing temporary file {sfp}: {e}")

    return out, scene_files


# Create dummy data for testing synthesis with emotional cues
test_scene_emotional = {
    'scene_id': 101,
    'lines': [
        {'character': 'Alice', 'text': "Wow, I'm so happy to see you!", 'emotional_cues': ['happy'], 'sound_effects': []},
        {'character': 'Bob', 'text': "Oh no, that's terrible news!", 'emotional_cues': ['sad'], 'sound_effects': []},
        {'character': 'Alice', 'text': "Get out of here!", 'emotional_cues': ['angry'], 'sound_effects': []},
        {'character': 'Bob', 'text': "I'm a bit scared of the dark.", 'emotional_cues': ['scared'], 'sound_effects': []},
        {'character': 'Narrator', 'text': "She spoke calmly.", 'emotional_cues': ['calm'], 'sound_effects': []}
    ],
    'scene_summary': 'Testing emotional synthesis',
    'emotional_cues': ['happy', 'sad', 'angry', 'scared', 'calm'],
    'sound_effects': []
}

test_char_map_emotional = {
    'Alice': {'voice': 'v2/en_speaker_6', 'gender':'female', 'emotion':'neutral'}, # Use a Bark voice
    'Bob': {'voice': 'v2/en_speaker_9', 'gender':'male', 'emotion':'neutral'},   # Use a Bark voice
    'Narrator': {'voice': 'neutral_default_gtts', 'gender':'neutral', 'emotion':'neutral'} # Use gTTS fallback voice
}

# Test synthesis with the emotional scene and Bark backend
print("\n--- Synthesizing test scene with Bark and emotional cues ---")
try:
    # Pass the emotional scene and char_map
    synthesized_audio_path_emotional_bark = synthesize_scene(test_scene_emotional, test_char_map_emotional, backend='bark')

    if synthesized_audio_path_emotional_bark and os.path.exists(synthesized_audio_path_emotional_bark):
        print(f"Emotional Bark synthesis complete. Output file: {synthesized_audio_path_emotional_bark}")
    elif synthesized_audio_path_emotional_bark is None:
         print("Emotional Bark synthesis failed or returned None.")
    else:
         print(f"Emotional Bark synthesis complete, but output file {synthesized_audio_path_emotional_bark} does not exist.")

except Exception as e:
    print(f"An error occurred during emotional Bark synthesis test: {e}")

print("\n--- Synthesizing test scene with gTTS fallback and emotional cues ---")
# Test synthesis with the emotional scene and gTTS backend
try:
    # Pass the emotional scene and char_map
    synthesized_audio_path_emotional_gtts = synthesize_scene(test_scene_emotional, test_char_map_emotional, backend='gTTS')

    if synthesized_audio_path_emotional_gtts and os.path.exists(synthesized_audio_path_emotional_gtts):
        print(f"Emotional gTTS fallback synthesis complete. Output file: {synthesized_audio_path_emotional_gtts}")
    elif synthesized_audio_path_emotional_gtts is None:
         print("Emotional gTTS fallback synthesis failed or returned None.")
    else:
         print(f"Emotional gTTS fallback synthesis complete, but output file {synthesized_audio_path_emotional_gtts} does not exist.")

except Exception as e:
    print(f"An error occurred during emotional gTTS fallback test: {e}")

"""## Add background audio and sound effects

### Subtask:
Implement logic to select and mix background music and sound effects (including animal sounds) based on the story's content and scene analysis. This might involve searching for and integrating sound effect libraries or APIs.

**Reasoning**:
Implement sound effect mixing logic in the `synthesize_scene` function and update the `mix_with_background` function if needed, then test with a scene containing sound effect keywords.
"""

import os
import uuid
from pydub import AudioSegment
import random # Needed to pick random sound files if multiple options exist

# Placeholder mapping of keywords to sound effect file paths
# In a real application, these would be actual paths to audio files (.mp3, .wav)
# You could organize sound effects by category (e.g., animal_sounds, weather_sfx)
SOUND_EFFECT_MAP = {
    'barked': '/content/dog_bark.mp3', # Placeholder - replace with actual file
    'howled': '/content/wolf_howl.mp3', # Placeholder
    'meowed': '/content/cat_meow.mp3', # Placeholder
    'roared': '/content/lion_roar.mp3', # Placeholder
    'hissed': '/content/snake_hiss.mp3', # Placeholder
    'wind': '/content/wind_sfx.mp3',   # Placeholder
    'rain': '/content/rain_sfx.mp3',   # Placeholder
    'thunder': '/content/thunder_sfx.mp3', # Placeholder
    'door creaked': '/content/door_creak_sfx.mp3', # Placeholder
    'footsteps': '/content/footsteps_sfx.mp3', # Placeholder
    # Add more keywords and placeholder paths as needed
}

# Create dummy sound effect files if they don't exist
# This is just to prevent errors during testing if the files aren't real
def create_dummy_sound_file(filepath, duration_ms=1000):
    if not os.path.exists(filepath):
        try:
            # Create a silent or simple tone audio segment
            dummy_audio = AudioSegment.silent(duration=duration_ms)
            dummy_audio.export(filepath, format=filepath.split('.')[-1])
            print(f"Created dummy sound effect file: {filepath}")
        except Exception as e:
            print(f"Could not create dummy sound file {filepath}: {e}")

# Create dummy sound files for the placeholders
for sfx_path in SOUND_EFFECT_MAP.values():
    create_dummy_sound_file(sfx_path)


# Modify synthesize_scene to check for and mix sound effects
def synthesize_scene(scene, char_map, backend='gTTS', music_path=None, intensity=0.3):
    """Synthesize all lines in a scene, mix sound effects, and combine them."""
    scene_segments = []
    tmp_files = []
    line_audio_paths = {} # Store audio path for each line to potentially align SFX

    for i, ln in enumerate(scene['lines']):
        char = ln['character']
        text = ln['text']
        emotion = ln.get('emotional_cues', [])
        emotion_str = emotion[0] if emotion else char_map.get(char, {}).get('emotion', 'neutral')

        backend_params = {'voice': char_map.get(char, {}).get('voice')}
        audio_path = synthesize_line_tts_backend(char, text, emotion=emotion_str, backend=backend, backend_params=backend_params)

        if audio_path and os.path.exists(audio_path):
            tmp_files.append(audio_path)
            line_audio_paths[i] = audio_path # Store path by line index
            try:
                seg = AudioSegment.from_file(audio_path)
                scene_segments.append(seg)
            except Exception as e:
                 print(f"Error loading audio segment from {audio_path}: {e}")
                 pass

    if not scene_segments:
        print(f"No audio segments generated for scene {scene['scene_id']}.")
        return None

    # concatenate speech segments
    combined_speech = AudioSegment.silent(duration=0)
    for i, seg in enumerate(scene_segments):
        combined_speech += seg
        if i < len(scene_segments) - 1:
            combined_speech += AudioSegment.silent(duration=300) # Small pause between lines

    # Mix sound effects
    final_scene_audio = combined_speech

    detected_sfx_keywords = scene.get('sound_effects', [])
    if detected_sfx_keywords:
        print(f"Detected sound effects for scene {scene['scene_id']}: {detected_sfx_keywords}")
        for keyword in detected_sfx_keywords:
            sfx_path = SOUND_EFFECT_MAP.get(keyword.lower()) # Get file path from map
            if sfx_path and os.path.exists(sfx_path):
                try:
                    sfx_audio = AudioSegment.from_file(sfx_path)
                    # Simple approach: overlay sound effect at the beginning of the scene
                    # More complex: try to align SFX with lines where keyword was found
                    # For simplicity, let's overlay at the start. You might adjust volume.
                    # Ensure SFX doesn't overpower speech
                    sfx_audio = sfx_audio - 6 # Reduce SFX volume by 6 dB as an example

                    # Overlay the sound effect. You might need to adjust timing.
                    # E.g., overlay at 0ms for start, or try to estimate timing.
                    # Simple overlay at the beginning:
                    final_scene_audio = final_scene_audio.overlay(sfx_audio, position=0) # Overlay at the start

                    print(f"Mixed sound effect '{keyword}' from {sfx_path} into scene {scene['scene_id']}.")

                except Exception as e:
                    print(f"Error mixing sound effect '{keyword}' from {sfx_path}: {e}")

    # export temp file
    outpath = f'/content/scene_{scene["scene_id"]}_{uuid.uuid4().hex}.mp3'
    try:
        final_scene_audio.export(outpath, format='mp3')
    except Exception as e:
        print(f"Error exporting combined scene audio to {outpath}: {e}")
        outpath = None

    # Clean up temporary individual line audio files
    for tf in tmp_files:
        try:
            os.remove(tf)
        except OSError as e:
            print(f"Error removing temporary file {tf}: {e}")

    # Mix music if requested and if combined audio was exported
    if music_path and outpath and os.path.exists(music_path):
        mixed = mix_with_background(outpath, music_path=music_path, intensity=intensity)
        # Clean up the combined speech+sfx file after mixing music
        try:
            os.remove(outpath)
        except OSError as e:
            print(f"Error removing temporary file {outpath}: {e}")
        return mixed
    elif outpath:
         return outpath
    else:
         return None


# The synthesize_full function should work without modification as it calls synthesize_scene

# Create a dummy scene with sound effect keywords for testing
test_scene_sfx = {
    'scene_id': 102,
    'lines': [
        {'character': 'Bob', 'text': "Listen, I hear thunder in the distance.", 'emotional_cues': ['worried'], 'sound_effects': ['thunder']},
        {'character': 'Alice', 'text': "And the dog barked!", 'emotional_cues': [], 'sound_effects': ['barked']},
        {'character': 'Narrator', 'text': "Footsteps echoed down the hall.", 'emotional_cues': [], 'sound_effects': ['footsteps']}
    ],
    'scene_summary': 'Scene with sound effects',
    'emotional_cues': ['worried'],
    'sound_effects': ['thunder', 'barked', 'footsteps'] # Ensure scene-level reflects line-level for testing
}

test_char_map_sfx = {
    'Alice': {'voice': 'female_default', 'gender':'female', 'emotion':'neutral'}, # Will use gTTS
    'Bob': {'voice': 'male_default', 'gender':'male', 'emotion':'neutral'},     # Will use gTTS
    'Narrator': {'voice': 'neutral_default_gtts', 'gender':'neutral', 'emotion':'neutral'} # Will use gTTS
}


print("\n--- Synthesizing test scene with Sound Effects (using gTTS) ---")
try:
    # Synthesize the scene with SFX using gTTS backend (since Bark is having issues)
    synthesized_audio_path_sfx_gtts = synthesize_scene(test_scene_sfx, test_char_map_sfx, backend='gTTS')

    if synthesized_audio_path_sfx_gtts and os.path.exists(synthesized_audio_path_sfx_gtts):
        print(f"Scene with SFX synthesis complete. Output file: {synthesized_audio_path_sfx_gtts}")
        # You would manually verify this audio file contains the speech and overlaid sound effects
    elif synthesized_audio_path_sfx_gtts is None:
         print("Scene with SFX synthesis failed or returned None.")
    else:
         print(f"Scene with SFX synthesis complete, but output file {synthesized_audio_path_sfx_gtts} does not exist.")

except Exception as e:
    print(f"An error occurred during SFX synthesis test: {e}")

"""**Reasoning**:
The previous step successfully implemented and tested the sound effect mixing logic within the `synthesize_scene` function using dummy sound files. The Gradio UI functions need to be updated to utilize the enhanced `synthesize_scene` and `synthesize_full` functions that now handle sound effects and pass the music file and intensity parameters correctly from the UI.


"""

import gradio as gr # Re-import gradio if kernel state reset

# Update the Gradio interface build function to ensure SFX and Music parameters are passed
def build_gradio_interface():
    with gr.Blocks() as demo:
        gr.Markdown('# PDF → Emotional Audioplay')
        with gr.Row():
            pdf_in = gr.File(label='Upload PDF', file_types=['.pdf'])
            # Keep 'gTTS' as default or choose 'bark' if confident in its loading
            model_choice = gr.Radio(choices=['gTTS (fast, low quality)','bark (expressive)','tortoise (high quality)','parler/xtts (emotion)'], value='gTTS (fast, low quality)', label='TTS Backend')
        out_text = gr.Textbox(label='Extracted text (preview)', lines=10)
        characters_json = gr.JSON(label='Detected Characters & Defaults')
        scene_dropdown = gr.Dropdown(label='Scenes', choices=[], value=None)
        scene_player = gr.Audio(label='Scene audio')
        combined_player = gr.Audio(label='Combined audio')
        music_upload = gr.File(label='Background music (optional)', file_types=['.mp3','.wav'])
        intensity = gr.Slider(minimum=0.0, maximum=1.0, value=0.3, label='Music intensity')
        generate_btn = gr.Button('Generate Script & Characters')
        synth_scene_btn = gr.Button('Generate Scene Audio')
        synth_full_btn = gr.Button('Generate Full Audio')

        state = gr.State({})

        def on_generate(pdf_file, model_choice_val):
            if not pdf_file:
                return 'Upload a PDF to begin.', {}, [], None, {} # Return empty state
            tmp_path = pdf_file.name
            result = process_pdf_and_prepare_script(tmp_path)
            # scene labels
            scenes = result['scenes']
            scene_labels = [f"Scene {s['scene_id']}: {(' '.join(s['scene_summary']))[:80]}" for s in scenes]
            return result['full_text'][:4000], result['char_map'], scene_labels, result # Pass the full result to state

        def on_synthesize_scene(selected_scene_label, state_obj, music_file, intensity_val, backend_choice):
            if not state_obj or not selected_scene_label:
                return None, state_obj # Return existing state

            scenes = state_obj['scenes']
            # map label back to scene index
            try:
                idx = int(selected_scene_label.split()[1].strip(':'))
                scene = scenes[idx]
            except (ValueError, IndexError):
                 print(f"Could not parse scene index from label: {selected_scene_label}")
                 return None, state_obj # Return existing state if index parsing fails

            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None

            # Call the enhanced synthesize_scene function
            out = synthesize_scene(scene, state_obj['char_map'], backend=backend, music_path=music_path, intensity=intensity_val)

            # Gradio expects a file path or None. Ensure out is a valid path if synthesis succeeded.
            if out and os.path.exists(out):
                return out, state_obj
            else:
                print("Scene synthesis failed or returned invalid path.")
                return None, state_obj # Return None audio and existing state on failure


        def on_synthesize_full(state_obj, music_file, intensity_val, backend_choice):
            if not state_obj:
                return None, None, state_obj # Return None audio, empty dataset, existing state

            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None

            # Call the enhanced synthesize_full function
            out, scene_files = synthesize_full(state_obj['scenes'], state_obj['char_map'], backend=backend, music_path=music_path, intensity=intensity_val)

            # Gradio expects a file path or None for combined audio
            # and a list of file paths for the Dataset component
            if out and os.path.exists(out):
                 # For the Dataset component, Gradio needs data. We can return the scene files list.
                 # However, the output component is gr.Dataset() which expects specific data types.
                 # A simple way is to just return the combined audio and leave the dataset empty
                 # or figure out how to format scene_files for gr.Dataset if needed.
                 # For now, let's just return the combined audio and the scene files list
                 # as a placeholder for potential Dataset display.
                 # Let's return the combined audio path and the list of scene files.
                 # The gr.Dataset() output might need adjustment in the outputs array.
                 # Let's assume gr.Dataset() can display a list of files.
                 return out, scene_files, state_obj
            elif out is None and scene_files:
                 # If combined audio failed but scene files exist, return None for combined and the list of scene files
                 return None, scene_files, state_obj
            else:
                 print("Full synthesis failed or returned no files.")
                 return None, [], state_obj # Return None audio, empty list, existing state on failure


        # Update button clicks to pass music_upload and intensity
        generate_btn.click(on_generate, inputs=[pdf_in, model_choice], outputs=[out_text, characters_json, scene_dropdown, state])
        # Ensure music_upload and intensity are passed to synth_scene_btn
        synth_scene_btn.click(on_synthesize_scene, inputs=[scene_dropdown, state, music_upload, intensity, model_choice], outputs=[scene_player, state])
        # Ensure music_upload and intensity are passed to synth_full_btn, and add state to outputs
        synth_full_btn.click(on_synthesize_full, inputs=[state, music_upload, intensity, model_choice], outputs=[combined_player, gr.Dataset(), state])


    return demo

# Rebuild the demo interface with updated callbacks
demo = build_gradio_interface()

# To run in Colab, use: demo.launch(share=True)
if __name__ == '__main__':
    # This block will only execute when the script is run directly, not in Gradio's server mode
    # For Colab, you would typically run demo.launch(share=True) in a separate cell
    print("Gradio demo interface built. Run demo.launch() to start the server.")


# Test with the dummy scene containing sound effects again, just to confirm functions are callable
print("\n--- Final test of synthesize_scene with SFX using gTTS ---")
try:
    # Re-use the test data defined in the previous step
    # test_scene_sfx, test_char_map_sfx are available from the previous execution

    synthesized_audio_path_sfx_gtts_final_test = synthesize_scene(test_scene_sfx, test_char_map_sfx, backend='gTTS')

    if synthesized_audio_path_sfx_gtts_final_test and os.path.exists(synthesized_audio_path_sfx_gtts_final_test):
        print(f"Final SFX test successful. Output file: {synthesized_audio_path_sfx_gtts_final_test}")
    elif synthesized_audio_path_sfx_gtts_final_test is None:
         print("Final SFX test failed or returned None.")
    else:
         print(f"Final SFX test complete, but output file {synthesized_audio_path_sfx_gtts_final_test} does not exist.")
except Exception as e:
    print(f"An error occurred during final SFX test: {e}")

"""## Update gradio interface

### Subtask:
Modify the Gradio UI to include options for selecting voices, emotions, and potentially uploading or selecting background audio/sound effects.

**Reasoning**:
Modify the Gradio UI to include elements for viewing/editing character-to-voice mapping and for handling sound effects. Update callbacks to manage this data in the state object.
"""

import gradio as gr
import pandas as pd
import json

# Re-define build_gradio_interface to include new UI elements
def build_gradio_interface():
    with gr.Blocks() as demo:
        gr.Markdown('# PDF → Emotional Audioplay')
        with gr.Row():
            pdf_in = gr.File(label='Upload PDF', file_types=['.pdf'])
            model_choice = gr.Radio(choices=['gTTS (fast, low quality)','bark (expressive)','tortoise (high quality)','parler/xtts (emotion)'], value='gTTS (fast, low quality)', label='TTS Backend')

        # New UI elements for character mapping and sound effects
        with gr.Accordion("Character Voice & Emotion Mapping", open=False):
            gr.Markdown("Edit the voice and emotion for each character. Voice options depend on the selected backend.")
            # Use Dataframe for character mapping - better for editing multiple characters
            characters_df = gr.Dataframe(label='Character Mapping',
                                         headers=['Character', 'Voice', 'Gender', 'Emotion'],
                                         datatype=['str', 'str', 'str', 'str'],
                                         interactive=True)
            # Placeholder for Voice options - will be populated dynamically
            voice_options_json = gr.JSON(label="Available Voice Options (Backend Dependent)")

        with gr.Accordion("Scene Details & Sound Effects", open=False):
            gr.Markdown("Review detected scenes and sound effects. You can't edit here, but this shows what was found.")
            scene_dropdown = gr.Dropdown(label='Select Scene to Preview Details', choices=[], value=None)
            scene_details_json = gr.JSON(label='Selected Scene Details (Lines, Cues, SFX)')
            scene_player = gr.Audio(label='Scene audio Preview')


        with gr.Accordion("Background Music & Additional Sound Effects", open=False):
             gr.Markdown("Add background music or manually trigger additional sound effects.")
             music_upload = gr.File(label='Background music (optional)', file_types=['.mp3','.wav'])
             intensity = gr.Slider(minimum=0.0, maximum=1.0, value=0.3, label='Music intensity')
             # Allow selecting additional sound effects to mix into the *current* scene preview
             # Could be a dropdown populated from SOUND_EFFECT_MAP
             available_sfx_choices = list(SOUND_EFFECT_MAP.keys())
             additional_sfx_dropdown = gr.Dropdown(label='Add Sound Effect to Scene Preview', choices=available_sfx_choices, allow_multiselect=True)


        # Original outputs and buttons
        out_text = gr.Textbox(label='Extracted text (preview)', lines=10)
        combined_player = gr.Audio(label='Combined Story Audio')
        generate_btn = gr.Button('1. Analyze PDF & Generate Script')
        synth_scene_btn = gr.Button('2. Synthesize Selected Scene Audio')
        synth_full_btn = gr.Button('3. Synthesize Full Story Audio')

        # State variable to hold the parsed data (characters, scenes, char_map)
        state = gr.State({})

        # Helper to get available voice options based on backend
        def get_voice_options(backend_choice_val):
            backend = backend_choice_val.split()[0].lower()
            # Define available voices/presets for each backend
            # These are placeholders; replace with actual lists of voices for each model
            voice_options = {
                'gtts': ['default'], # gTTS doesn't support distinct voices this way
                'bark': ['v2/en_speaker_0', 'v2/en_speaker_6', 'v2/en_speaker_9', 'v2/fr_speaker_0', 'v2/es_speaker_0'], # Example Bark presets
                'tortoise': ['angie', 'daniel', 'emma', 'freya', 'geralt', 'harry', 'ljspeech', 'mary_ann', 'michal', 'patricia', 'rainbow', 'roger', 'tom'], # Example Tortoise voices
                'parler/xtts': ['female_default', 'male_default'] # Placeholder XTTS voices
            }
            return voice_options.get(backend, ['default']) # Return default if backend not found


        def on_generate(pdf_file, model_choice_val):
            if not pdf_file:
                # Clear outputs if no file uploaded
                return 'Upload a PDF to begin.', pd.DataFrame(columns=['Character', 'Voice', 'Gender', 'Emotion']), [], None, {}, {}

            tmp_path = pdf_file.name
            result = process_pdf_and_prepare_script(tmp_path)

            # Prepare characters_df from char_map
            char_map_df = pd.DataFrame([(k, v['voice'], v['gender'], v['emotion']) for k, v in result['char_map'].items()],
                                       columns=['Character', 'Voice', 'Gender', 'Emotion'])

            # Prepare scene labels for dropdown
            scenes = result['scenes']
            scene_labels = [f"Scene {s['scene_id']}: {(' '.join(s['scene_summary']))[:80]}" for s in scenes]

            # Get available voice options for display
            available_voices = get_voice_options(model_choice_val)


            # Store the full result in the state
            return result['full_text'][:4000], char_map_df, scene_labels, None, result, available_voices # Return None for scene_details_json initially


        def on_scene_select(selected_scene_label, state_obj):
             if not state_obj or not selected_scene_label:
                 return None # Return None if no state or scene selected

             scenes = state_obj.get('scenes', [])
             try:
                 idx = int(selected_scene_label.split()[1].strip(':'))
                 # Find the scene by index
                 scene = next((s for s in scenes if s['scene_id'] == idx), None)
                 return scene # Return the selected scene object (will be displayed as JSON)
             except (ValueError, IndexError, StopIteration):
                  print(f"Could not find scene for label: {selected_scene_label}")
                  return None


        def on_synthesize_scene(selected_scene_label, current_char_df, state_obj, music_file, intensity_val, backend_choice, additional_sfx_keywords):
            if not state_obj or not selected_scene_label:
                return None, state_obj # Return existing state if no state or scene selected

            scenes = state_obj.get('scenes', [])
            try:
                idx = int(selected_scene_label.split()[1].strip(':'))
                scene_to_synth = next((s for s in scenes if s['scene_id'] == idx), None)
                if not scene_to_synth:
                     print(f"Scene {idx} not found in state.")
                     return None, state_obj
            except (ValueError, IndexError, StopIteration):
                 print(f"Could not parse or find scene for label: {selected_scene_label}")
                 return None, state_obj

            # Update the char_map in the state object based on user edits in the dataframe
            updated_char_map = {}
            if current_char_df is not None:
                for index, row in current_char_df.iterrows():
                    char_name = row['Character']
                    updated_char_map[char_name] = {
                        'voice': row['Voice'],
                        'gender': row['Gender'],
                        'emotion': row['Emotion'] # Emotion editing is basic here, could be a dropdown
                    }
                # Update state object's char_map
                state_obj['char_map'] = updated_char_map
            else:
                 # Use the original char_map from state if dataframe is None
                 updated_char_map = state_obj.get('char_map', {})


            # Add manually selected additional sound effects to the scene object for synthesis
            scene_to_synth_with_sfx = scene_to_synth.copy() # Create a copy to avoid modifying state directly for now
            if additional_sfx_keywords:
                 # Ensure the sound_effects list exists and add the keywords
                 scene_to_synth_with_sfx['sound_effects'] = list(set(scene_to_synth_with_sfx.get('sound_effects', []) + additional_sfx_keywords))
                 print(f"Synthesizing scene {idx} with additional SFX: {additional_sfx_keywords}")


            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None

            # Call synthesize_scene with the potentially updated char_map and modified scene object
            out = synthesize_scene(scene_to_synth_with_sfx, updated_char_map, backend=backend, music_path=music_path, intensity=intensity_val)

            # Gradio expects a file path or None. Ensure out is a valid path if synthesis succeeded.
            if out and os.path.exists(out):
                return out, state_obj # Return audio path and updated state
            else:
                print("Scene synthesis failed or returned invalid path.")
                return None, state_obj # Return None audio and existing state on failure


        def on_synthesize_full(current_char_df, state_obj, music_file, intensity_val, backend_choice):
            if not state_obj:
                return None, None, state_obj # Return None audio, empty dataset, existing state

            # Update the char_map in the state object based on user edits in the dataframe
            updated_char_map = {}
            if current_char_df is not None:
                for index, row in current_char_df.iterrows():
                    char_name = row['Character']
                    updated_char_map[char_name] = {
                        'voice': row['Voice'],
                        'gender': row['Gender'],
                        'emotion': row['Emotion']
                    }
                # Update state object's char_map
                state_obj['char_map'] = updated_char_map
            else:
                 updated_char_map = state_obj.get('char_map', {})


            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None

            # Call synthesize_full with the potentially updated char_map
            out, scene_files = synthesize_full(state_obj.get('scenes', []), updated_char_map, backend=backend, music_path=music_path, intensity=intensity_val)

            # Gradio expects a file path or None for combined audio
            # and a list of file paths for the Dataset component
            if out and os.path.exists(out):
                 return out, scene_files, state_obj # Return combined audio path, list of scene files, and updated state
            elif out is None and scene_files:
                 # If combined audio failed but scene files exist, return None for combined and the list of scene files
                 return None, scene_files, state_obj
            else:
                 print("Full synthesis failed or returned no files.")
                 return None, [], state_obj # Return None audio, empty list, existing state on failure


        # Connect events:
        # on_generate populates the characters_df, scene_dropdown, out_text, state, and voice_options_json
        generate_btn.click(on_generate,
                           inputs=[pdf_in, model_choice],
                           outputs=[out_text, characters_df, scene_dropdown, scene_details_json, state, voice_options_json]) # Added scene_details_json and voice_options_json to outputs

        # When a scene is selected from the dropdown, display its details
        scene_dropdown.change(on_scene_select,
                              inputs=[scene_dropdown, state],
                              outputs=[scene_details_json])

        # on_synthesize_scene uses characters_df, state, music_upload, intensity, model_choice, additional_sfx_dropdown
        # It updates scene_player and state
        synth_scene_btn.click(on_synthesize_scene,
                              inputs=[scene_dropdown, characters_df, state, music_upload, intensity, model_choice, additional_sfx_dropdown],
                              outputs=[scene_player, state])

        # on_synthesize_full uses characters_df, state, music_upload, intensity, model_choice
        # It updates combined_player, gr.Dataset(), and state
        synth_full_btn.click(on_synthesize_full,
                             inputs=[characters_df, state, music_upload, intensity, model_choice],
                             outputs=[combined_player, gr.Dataset(), state]) # Outputting scene_files list to gr.Dataset()

        # Update available voice options when model choice changes
        model_choice.change(lambda x: gr.update(value=get_voice_options(x)),
                            inputs=[model_choice],
                            outputs=[voice_options_json])


    return demo

# Rebuild the demo interface with updated UI and logic
demo = build_gradio_interface()

# The demo.launch() call should be in a separate cell to run the Gradio app
# demo.launch()

print("Gradio demo interface built with character mapping and SFX options.")

"""**Reasoning**:
The traceback indicates that the `gr.Dropdown` component in the installed Gradio version (3.40.0) does not support the `allow_multiselect` keyword argument. I need to modify the code to use a different approach for selecting multiple sound effects, or check if the Gradio version supports this in a different way. A common alternative for multi-selection in older Gradio versions is `gr.CheckboxGroup`.


"""

import gradio as gr
import pandas as pd
import json
import os # Ensure os is imported for file checks

# Re-define build_gradio_interface, replacing gr.Dropdown(allow_multiselect=True) with gr.CheckboxGroup
def build_gradio_interface():
    with gr.Blocks() as demo:
        gr.Markdown('# PDF → Emotional Audioplay')
        with gr.Row():
            pdf_in = gr.File(label='Upload PDF', file_types=['.pdf'])
            model_choice = gr.Radio(choices=['gTTS (fast, low quality)','bark (expressive)','tortoise (high quality)','parler/xtts (emotion)'], value='gTTS (fast, low quality)', label='TTS Backend')

        # New UI elements for character mapping and sound effects
        with gr.Accordion("Character Voice & Emotion Mapping", open=False):
            gr.Markdown("Edit the voice and emotion for each character. Voice options depend on the selected backend.")
            # Use Dataframe for character mapping - better for editing multiple characters
            characters_df = gr.Dataframe(label='Character Mapping',
                                         headers=['Character', 'Voice', 'Gender', 'Emotion'],
                                         datatype=['str', 'str', 'str', 'str'],
                                         interactive=True)
            # Placeholder for Voice options - will be populated dynamically
            voice_options_json = gr.JSON(label="Available Voice Options (Backend Dependent)")

        with gr.Accordion("Scene Details & Sound Effects", open=False):
            gr.Markdown("Review detected scenes and sound effects. You can't edit here, but this shows what was found.")
            scene_dropdown = gr.Dropdown(label='Select Scene to Preview Details', choices=[], value=None)
            scene_details_json = gr.JSON(label='Selected Scene Details (Lines, Cues, SFX)')
            scene_player = gr.Audio(label='Scene audio Preview')


        with gr.Accordion("Background Music & Additional Sound Effects", open=False):
             gr.Markdown("Add background music or manually trigger additional sound effects.")
             music_upload = gr.File(label='Background music (optional)', file_types=['.mp3','.wav'])
             intensity = gr.Slider(minimum=0.0, maximum=1.0, value=0.3, label='Music intensity')
             # Allow selecting additional sound effects to mix into the *current* scene preview
             # Replaced gr.Dropdown(allow_multiselect=True) with gr.CheckboxGroup
             available_sfx_choices = list(SOUND_EFFECT_MAP.keys())
             additional_sfx_checkboxes = gr.CheckboxGroup(label='Add Sound Effects to Scene Preview', choices=available_sfx_choices)


        # Original outputs and buttons
        out_text = gr.Textbox(label='Extracted text (preview)', lines=10)
        combined_player = gr.Audio(label='Combined Story Audio')
        generate_btn = gr.Button('1. Analyze PDF & Generate Script')
        synth_scene_btn = gr.Button('2. Synthesize Selected Scene Audio')
        synth_full_btn = gr.Button('3. Synthesize Full Story Audio')

        # State variable to hold the parsed data (characters, scenes, char_map)
        state = gr.State({})

        # Helper to get available voice options based on backend
        def get_voice_options(backend_choice_val):
            backend = backend_choice_val.split()[0].lower()
            # Define available voices/presets for each backend
            # These are placeholders; replace with actual lists of voices for each model
            voice_options = {
                'gtts': ['default'], # gTTS doesn't support distinct voices this way
                'bark': ['v2/en_speaker_0', 'v2/en_speaker_6', 'v2/en_speaker_9', 'v2/fr_speaker_0', 'v2/es_speaker_0'], # Example Bark presets
                'tortoise': ['angie', 'daniel', 'emma', 'freya', 'geralt', 'harry', 'ljspeech', 'mary_ann', 'michal', 'patricia', 'rainbow', 'roger', 'tom'], # Example Tortoise voices
                'parler/xtts': ['female_default', 'male_default'] # Placeholder XTTS voices
            }
            return voice_options.get(backend, ['default']) # Return default if backend not found


        def on_generate(pdf_file, model_choice_val):
            if not pdf_file:
                # Clear outputs if no file uploaded
                # Need to return a list of outputs matching the click function's outputs
                # outputs=[out_text, characters_df, scene_dropdown, scene_details_json, state, voice_options_json]
                return 'Upload a PDF to begin.', pd.DataFrame(columns=['Character', 'Voice', 'Gender', 'Emotion']), [], None, {}, get_voice_options(model_choice_val) # Return empty state and default voice options

            tmp_path = pdf_file.name
            result = process_pdf_and_prepare_script(tmp_path)

            # Prepare characters_df from char_map
            char_map_df = pd.DataFrame([(k, v['voice'], v['gender'], v['emotion']) for k, v in result['char_map'].items()],
                                       columns=['Character', 'Voice', 'Gender', 'Emotion'])

            # Prepare scene labels for dropdown
            scenes = result['scenes']
            scene_labels = [f"Scene {s['scene_id']}: {(' '.join(s['scene_summary']))[:80]}" for s in scenes]

            # Get available voice options for display
            available_voices = get_voice_options(model_choice_val)


            # Store the full result in the state
            return result['full_text'][:4000], char_map_df, scene_labels, None, result, available_voices # Return None for scene_details_json initially


        def on_scene_select(selected_scene_label, state_obj):
             if not state_obj or not selected_scene_label:
                 return None # Return None if no state or scene selected

             scenes = state_obj.get('scenes', [])
             try:
                 idx = int(selected_scene_label.split()[1].strip(':'))
                 # Find the scene by index
                 scene = next((s for s in scenes if s['scene_id'] == idx), None)
                 # Return the selected scene object (will be displayed as JSON)
                 # Gradio JSON component expects a JSON serializable object.
                 # Ensure the scene object is serializable.
                 return json.loads(json.dumps(scene)) if scene else None
             except (ValueError, IndexError, StopIteration, TypeError) as e: # Added TypeError for json.dumps issues
                  print(f"Could not find scene for label or serialize scene: {selected_scene_label}, Error: {e}")
                  return None


        def on_synthesize_scene(selected_scene_label, current_char_df, state_obj, music_file, intensity_val, backend_choice, additional_sfx_keywords):
            if not state_obj or not selected_scene_label:
                return None, state_obj # Return existing state if no state or scene selected

            scenes = state_obj.get('scenes', [])
            try:
                idx = int(selected_scene_label.split()[1].strip(':'))
                scene_to_synth = next((s for s in scenes if s['scene_id'] == idx), None)
                if not scene_to_synth:
                     print(f"Scene {idx} not found in state.")
                     return None, state_obj
            except (ValueError, IndexError, StopIteration):
                 print(f"Could not parse or find scene for label: {selected_scene_label}")
                 return None, state_obj

            # Update the char_map in the state object based on user edits in the dataframe
            updated_char_map = {}
            if current_char_df is not None:
                for index, row in current_char_df.iterrows():
                    char_name = row['Character']
                    # Ensure row values are treated as strings to avoid potential errors
                    updated_char_map[char_name] = {
                        'voice': str(row['Voice']) if pd.notna(row['Voice']) else '',
                        'gender': str(row['Gender']) if pd.notna(row['Gender']) else '',
                        'emotion': str(row['Emotion']) if pd.notna(row['Emotion']) else '' # Emotion editing is basic here, could be a dropdown
                    }
                # Update state object's char_map
                state_obj['char_map'] = updated_char_map
            else:
                 # Use the original char_map from state if dataframe is None
                 updated_char_map = state_obj.get('char_map', {})


            # Add manually selected additional sound effects to the scene object for synthesis
            # Create a deep copy to avoid modifying the original scene object in state
            import copy
            scene_to_synth_with_sfx = copy.deepcopy(scene_to_synth)

            if additional_sfx_keywords:
                 # Ensure the sound_effects list exists and add the keywords
                 scene_to_synth_with_sfx['sound_effects'] = list(set(scene_to_synth_with_sfx.get('sound_effects', []) + additional_sfx_keywords))
                 print(f"Synthesizing scene {idx} with additional SFX: {additional_sfx_keywords}")
            # else: use the sound_effects detected during initial processing


            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None if music_file else None # Handle None case for music_file


            # Call synthesize_scene with the potentially updated char_map and modified scene object
            out = synthesize_scene(scene_to_synth_with_sfx, updated_char_map, backend=backend, music_path=music_path, intensity=intensity_val)

            # Gradio expects a file path or None. Ensure out is a valid path if synthesis succeeded.
            if out and os.path.exists(out):
                return out, state_obj # Return audio path and updated state
            else:
                print("Scene synthesis failed or returned invalid path.")
                return None, state_obj # Return None audio and existing state on failure


        def on_synthesize_full(current_char_df, state_obj, music_file, intensity_val, backend_choice):
            if not state_obj:
                return None, None, state_obj # Return None audio, empty dataset, existing state

            # Update the char_map in the state object based on user edits in the dataframe
            updated_char_map = {}
            if current_char_df is not None:
                for index, row in current_char_df.iterrows():
                     char_name = row['Character']
                     # Ensure row values are treated as strings to avoid potential errors
                     updated_char_map[char_name] = {
                         'voice': str(row['Voice']) if pd.notna(row['Voice']) else '',
                         'gender': str(row['Gender']) if pd.notna(row['Gender']) else '',
                         'emotion': str(row['Emotion']) if pd.notna(row['Emotion']) else ''
                     }
                # Update state object's char_map
                state_obj['char_map'] = updated_char_map
            else:
                 updated_char_map = state_obj.get('char_map', {})


            backend = backend_choice.split()[0].lower()
            music_path = music_file.name if music_file else None


            # Call synthesize_full with the potentially updated char_map
            out, scene_files = synthesize_full(state_obj.get('scenes', []), updated_char_map, backend=backend, music_path=music_path, intensity=intensity_val)

            # Gradio expects a file path or None for combined audio
            # and a list of file paths for the Dataset component
            if out and os.path.exists(out):
                 # Return combined audio path, list of scene files, and updated state
                 # Gradio's gr.Dataset expects a list of lists or similar structure.
                 # Returning a list of files might work directly, or need formatting.
                 # Let's try returning the list of file paths directly.
                 return out, [[f] for f in scene_files], state_obj # Format for gr.Dataset
            elif out is None and scene_files:
                 # If combined audio failed but scene files exist, return None for combined and the list of scene files
                 return None, [[f] for f in scene_files], state_obj # Format for gr.Dataset
            else:
                 print("Full synthesis failed or returned no files.")
                 return None, [], state_obj # Return None audio, empty list, existing state on failure


        # Connect events:
        # on_generate populates the characters_df, scene_dropdown, out_text, state, and voice_options_json
        # Updated outputs to match the function return
        generate_btn.click(on_generate,
                           inputs=[pdf_in, model_choice],
                           outputs=[out_text, characters_df, scene_dropdown, scene_details_json, state, voice_options_json])

        # When a scene is selected from the dropdown, display its details
        scene_dropdown.change(on_scene_select,
                              inputs=[scene_dropdown, state],
                              outputs=[scene_details_json])

        # on_synthesize_scene uses scene_dropdown, characters_df, state, music_upload, intensity, model_choice, additional_sfx_checkboxes
        # It updates scene_player and state
        synth_scene_btn.click(on_synthesize_scene,
                              inputs=[scene_dropdown, characters_df, state, music_upload, intensity, model_choice, additional_sfx_checkboxes],
                              outputs=[scene_player, state])

        # on_synthesize_full uses characters_df, state, music_upload, intensity, model_choice
        # It updates combined_player, gr.Dataset(), and state
        # Updated inputs and outputs to match the function signature and returns
        synth_full_btn.click(on_synthesize_full,
                             inputs=[characters_df, state, music_upload, intensity, model_choice],
                             outputs=[combined_player, gr.Dataset(), state])

        # Update available voice options when model choice changes
        # The lambda function now returns the dictionary of available voices, which the JSON component can display.
        model_choice.change(lambda x: get_voice_options(x),
                            inputs=[model_choice],
                            outputs=[voice_options_json])


    return demo

# Rebuild the demo interface with updated UI and logic
demo = build_gradio_interface()

# The demo.launch() call should be in a separate cell to run the Gradio app
# demo.launch()

print("Gradio demo interface built with character mapping and SFX options.")

"""## Synthesize and combine audio

### Subtask:
Update the synthesis functions to generate audio for each line/scene, incorporating the chosen voices, emotions, background audio, and sound effects, and then combine them into a final audio play.

**Reasoning**:
Review and update the synthesis functions to incorporate background audio and sound effects, ensuring the correct mixing logic and file handling.
"""

import os
import uuid
from pydub import AudioSegment
import random
import soundfile as sf
import numpy as np
import torch
from transformers import BarkModel, AutoTokenizer
from gtts import gTTS # Ensure gTTS is imported here

# Check for GPU and set device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load Bark model and tokenizer
# Using a try-except block to handle potential loading errors
try:
    # Explicitly specify device when loading
    bark_tokenizer = AutoTokenizer.from_pretrained("suno/bark")
    bark_model = BarkModel.from_pretrained("suno/bark").to(device)
    print("Bark model and tokenizer loaded successfully.")
except Exception as e:
    print(f"Error loading Bark model: {e}")
    bark_tokenizer = None
    bark_model = None
    print("Bark model loading failed. Bark synthesis will not be available.")

# Fallback TTS using gTTS
def synthesize_with_gtts(text, lang='en'):
    """Synthesize text using Google Text-to-Speech."""
    try:
        tts = gTTS(text=text, lang=lang)
        tmpfile = f'/content/{uuid.uuid4().hex}.mp3'
        tts.save(tmpfile)
        return tmpfile
    except Exception as e:
        print(f"Error during gTTS synthesis: {e}")
        return None # Return None if gTTS fails

# Mix background music with ducking
def mix_with_background(speech_path, music_path=None, intensity=0.3):
    """Mix speech audio with background music."""
    try:
        speech = AudioSegment.from_file(speech_path)
        if not music_path or not os.path.exists(music_path):
            return speech # Return speech only if no music or music file not found

        music = AudioSegment.from_file(music_path)
        # make music match duration
        if len(music) < len(speech):
            # loop music
            times = int(len(speech) / len(music)) + 1
            music = music * times
        music = music[:len(speech)]
        # duck music
        music = music - (10 * (1 - intensity))  # simple attenuation
        mixed = speech.overlay(music)
        outpath = f'/content/{uuid.uuid4().hex}_mixed.mp3'
        mixed.export(outpath, format='mp3')
        return outpath
    except Exception as e:
        print(f"Error mixing audio with background: {e}")
        return speech_path # Return original speech path on error

# Placeholder mapping of keywords to sound effect file paths
# Ensure this map is accessible
SOUND_EFFECT_MAP = {
    'barked': '/content/dog_bark.mp3', # Placeholder - replace with actual file
    'howled': '/content/wolf_howl.mp3', # Placeholder
    'meowed': '/content/cat_meow.mp3', # Placeholder
    'roared': '/content/lion_roar.mp3', # Placeholder
    'hissed': '/content/snake_hiss.mp3', # Placeholder
    'wind': '/content/wind_sfx.mp3',   # Placeholder
    'rain': '/content/rain_sfx.mp3',   # Placeholder
    'thunder': '/content/thunder_sfx.mp3', # Placeholder
    'door creaked': '/content/door_creak_sfx.mp3', # Placeholder
    'footsteps': '/content/footsteps_sfx.mp3', # Placeholder
    # Add more keywords and placeholder paths as needed
}

# Create dummy sound effect files if they don't exist (re-include for robustness)
def create_dummy_sound_file(filepath, duration_ms=1000):
    if not os.path.exists(filepath):
        try:
            # Create a silent or simple tone audio segment
            dummy_audio = AudioSegment.silent(duration=duration_ms)
            dummy_audio.export(filepath, format=filepath.split('.')[-1])
            print(f"Created dummy sound effect file: {filepath}")
        except Exception as e:
            print(f"Could not create dummy sound file {filepath}: {e}")

# Create dummy sound files for the placeholders
for sfx_path in SOUND_EFFECT_MAP.values():
    create_dummy_sound_file(sfx_path)


# Update the synthesize_line_tts_backend function
def synthesize_line_tts_backend(character, text, emotion='neutral', backend='gTTS', backend_params=None):
    """Synthesize a single line to an audio file path using chosen backend."""
    backend_params = backend_params or {}
    selected_voice = backend_params.get('voice') # Get the selected voice from char_map

    if backend == 'bark' and bark_model is not None and bark_tokenizer is not None:
        # print(f"Attempting Bark synthesis for character {character} with voice '{selected_voice}' and emotion '{emotion}': {text[:50]}...") # Keep print for debugging
        try:
            # Incorporate emotional cue and voice into the text prompt for Bark
            text_prompt_parts = []
            if selected_voice and selected_voice.startswith('v2/'):
                 text_prompt_parts.append(f"[{selected_voice}]")
            if emotion and emotion != 'neutral':
                 text_prompt_parts.append(f"[{emotion}]")

            text_prompt_parts.append(text)
            text_prompt = " ".join(text_prompt_parts)

            # Tokenize text
            inputs = bark_tokenizer(text_prompt, return_tensors="pt").to(device)

            # Generate audio
            with torch.no_grad():
                speech_output = bark_model.generate(**inputs, do_sample=True, temperature=0.7,
                                                    output_attentions=False, output_hidden_states=False)

            # Convert tensor to numpy array
            audio_array = speech_output[0].cpu().numpy()

            # Save to a temporary WAV file (Bark outputs raw audio)
            tmpfile_wav = f'/content/{uuid.uuid4().hex}.wav'
            sf.write(tmpfile_wav, audio_array, bark_model.generation_config.sample_rate)

            # Convert WAV to MP3
            tmpfile_mp3 = f'/content/{uuid.uuid4().hex}.mp3'
            AudioSegment.from_wav(tmpfile_wav).export(tmpfile_mp3, format="mp3")

            # Clean up the temporary WAV file
            os.remove(tmpfile_wav)

            # print(f"Bark synthesis successful. Output: {tmpfile_mp3}") # Keep print for debugging
            return tmpfile_mp3
        except Exception as e:
            print(f"Error synthesizing with Bark: {e}")
            # Fallback to gTTS if Bark fails
            print(f"Falling back to gTTS for character {character}.")
            return synthesize_with_gtts(text)

    elif backend == 'gTTS':
        # print(f"Using gTTS for character {character}: {text[:50]}...") # Keep print for debugging
        return synthesize_with_gtts(text)

    elif backend == 'tortoise':
        # Tortoise implementation placeholder
        print(f"Tortoise backend selected but not implemented. Using gTTS for character {character}: {text[:50]}...")
        # Placeholder: Ideally, Tortoise would use the selected_voice parameter
        # as a speaker ID or reference audio path and potentially emotion prompts.
        return synthesize_with_gtts(f"[{character} - {emotion}] {text}")
    else:
        print(f"Unknown backend '{backend}'. Using gTTS for character {character}: {text[:50]}...")
        return synthesize_with_gtts(text)


# Update synthesize_scene to handle SFX mixing and temporary file cleanup
def synthesize_scene(scene, char_map, backend='gTTS', music_path=None, intensity=0.3):
    """Synthesize all lines in a scene, mix sound effects, and combine them."""
    scene_segments = []
    tmp_line_audio_files = []
    line_audio_paths = {} # Store audio path for each line to potentially align SFX

    for i, ln in enumerate(scene['lines']):
        char = ln['character']
        text = ln['text']
        emotion = ln.get('emotional_cues', [])
        # Use the first emotional cue from the line if available, otherwise fallback to character map or 'neutral'
        emotion_str = emotion[0] if emotion else char_map.get(char, {}).get('emotion', 'neutral')

        backend_params = {'voice': char_map.get(char, {}).get('voice')}
        # Call the updated synthesize_line_tts_backend, passing the determined emotion_str
        audio_path = synthesize_line_tts_backend(char, text, emotion=emotion_str, backend=backend, backend_params=backend_params)

        if audio_path and os.path.exists(audio_path): # Only process if synthesis was successful
            tmp_line_audio_files.append(audio_path)
            line_audio_paths[i] = audio_path # Store path by line index
            try:
                seg = AudioSegment.from_file(audio_path)
                scene_segments.append(seg)
            except Exception as e:
                 print(f"Error loading audio segment from {audio_path}: {e}")
                 pass # Skipping the problematic segment for now

    # Clean up temporary individual line audio files *before* concatenating
    # This is safer as we already have the segments in memory
    for tf in tmp_line_audio_files:
        try:
            os.remove(tf)
        except OSError as e:
            print(f"Error removing temporary line file {tf}: {e}")


    if not scene_segments:
        print(f"No audio segments generated for scene {scene['scene_id']}.")
        return None # Return None if no audio was generated

    # concatenate speech segments
    combined_speech = AudioSegment.silent(duration=0)
    for i, seg in enumerate(scene_segments):
        combined_speech += seg
        if i < len(scene_segments) - 1: # Add pause only between segments
            combined_speech += AudioSegment.silent(duration=300) # Small pause between lines

    # Mix sound effects
    final_scene_audio = combined_speech

    detected_sfx_keywords = scene.get('sound_effects', [])
    if detected_sfx_keywords:
        print(f"Detected sound effects for scene {scene['scene_id']}: {detected_sfx_keywords}")
        for keyword in detected_sfx_keywords:
            sfx_path = SOUND_EFFECT_MAP.get(keyword.lower()) # Get file path from map
            if sfx_path and os.path.exists(sfx_path):
                try:
                    sfx_audio = AudioSegment.from_file(sfx_path)
                    # Simple approach: overlay sound effect at the beginning of the scene
                    sfx_audio = sfx_audio - 6 # Reduce SFX volume by 6 dB as an example

                    # Overlay the sound effect at the start (position=0)
                    final_scene_audio = final_scene_audio.overlay(sfx_audio, position=0)

                    print(f"Mixed sound effect '{keyword}' from {sfx_path} into scene {scene['scene_id']}.")

                except Exception as e:
                    print(f"Error mixing sound effect '{keyword}' from {sfx_path}: {e}")

    # export combined speech + sfx to a temporary file for mixing with music
    tmp_combined_speech_sfx_path = f'/content/scene_sfx_{scene["scene_id"]}_{uuid.uuid4().hex}.mp3'
    try:
        final_scene_audio.export(tmp_combined_speech_sfx_path, format='mp3')
    except Exception as e:
        print(f"Error exporting combined speech+sfx audio to {tmp_combined_speech_sfx_path}: {e}")
        tmp_combined_speech_sfx_path = None # Indicate export failed


    # Mix music if requested and if combined speech+sfx audio was exported
    if music_path and tmp_combined_speech_sfx_path and os.path.exists(music_path):
        mixed = mix_with_background(tmp_combined_speech_sfx_path, music_path=music_path, intensity=intensity)
        # Clean up the combined speech+sfx file after mixing music
        try:
            os.remove(tmp_combined_speech_sfx_path)
        except OSError as e:
            print(f"Error removing temporary file {tmp_combined_speech_sfx_path}: {e}")
        return mixed # Return the path to the final mixed audio
    elif tmp_combined_speech_sfx_path:
         return tmp_combined_speech_sfx_path # Return combined speech+sfx path if no music or mixing failed
    else:
         return None # Return None if combined speech+sfx audio export failed


# Update synthesize_full to handle temporary scene audio file cleanup
def synthesize_full(scenes, char_map, backend='gTTS', music_path=None, intensity=0.25):
    """Synthesize all scenes and combine into a full story audio."""
    scene_audio_files = []
    tmp_scene_files = [] # Keep track of temporary scene audio files

    for s in scenes:
        # Pass the backend correctly
        # synthesize_scene already handles SFX and music mixing within the scene
        f = synthesize_scene(s, char_map, backend=backend, music_path=music_path, intensity=intensity)
        if f and os.path.exists(f): # Only add if scene synthesis was successful
            scene_audio_files.append(f)
            tmp_scene_files.append(f) # Add to list for cleanup


    if not scene_audio_files:
        print("No scene audio files generated for the full script.")
        return None, [] # Return None and empty list if no scenes were synthesized

    # combine into one large audio
    combined_story_audio = AudioSegment.silent(duration=0)
    for i, sfp in enumerate(scene_audio_files):
        try:
            combined_story_audio += AudioSegment.from_file(sfp)
            if i < len(scene_audio_files) - 1: # Add pause only between scenes
                 combined_story_audio += AudioSegment.silent(duration=500) # Pause between scenes
        except Exception as e:
             print(f"Error loading scene audio file {sfp}: {e}")
             pass # Skip problematic scene file

    if combined_story_audio.duration_seconds == 0:
         print("Combined audio is empty.")
         # Clean up temporary scene audio files even if combined audio is empty
         for sfp in tmp_scene_files:
            try:
                os.remove(sfp)
            except OSError as e:
                print(f"Error removing temporary scene file {sfp}: {e}")
         return None, [] # Return None and empty list if combined audio is empty


    out = f'/content/combined_story_{uuid.uuid4().hex}.mp3'
    try:
        combined_story_audio.export(out, format='mp3')
    except Exception as e:
        print(f"Error exporting combined story audio to {out}: {e}")
        out = None # Indicate export failed


    # Clean up temporary scene audio files after combining
    for sfp in tmp_scene_files:
        try:
            os.remove(sfp)
        except OSError as e:
            print(f"Error removing temporary scene file {sfp}: {e}")

    # Return the path to the final story audio and the list of temporary scene files (for Gradio Dataset)
    return out, tmp_scene_files


# Test cases (re-run with updated functions)
# Use the test data defined in previous steps

# Test with the dummy scene containing sound effects using gTTS
print("\n--- Testing synthesize_scene with SFX (using gTTS) ---")
try:
    # test_scene_sfx, test_char_map_sfx are available from previous execution
    synthesized_audio_path_sfx_gtts_test = synthesize_scene(test_scene_sfx, test_char_map_sfx, backend='gTTS')

    if synthesized_audio_path_sfx_gtts_test and os.path.exists(synthesized_audio_path_sfx_gtts_test):
        print(f"SFX synthesis test complete. Output file: {synthesized_audio_path_sfx_gtts_test}")
        # Clean up the test output file
        try:
            os.remove(synthesized_audio_path_sfx_gtts_test)
            print(f"Cleaned up test file: {synthesized_audio_path_sfx_gtts_test}")
        except OSError as e:
            print(f"Error removing test file {synthesized_audio_path_sfx_gtts_test}: {e}")

    elif synthesized_audio_path_sfx_gtts_test is None:
         print("SFX synthesis test failed or returned None.")
    else:
         print(f"SFX synthesis test complete, but output file {synthesized_audio_path_sfx_gtts_test} does not exist.")

except Exception as e:
    print(f"An error occurred during SFX synthesis test: {e}")


# Test with the dummy scene containing emotional cues using Bark (will likely fallback to gTTS)
print("\n--- Testing synthesize_scene with Emotional Cues (using Bark/gTTS fallback) ---")
try:
    # test_scene_emotional, test_char_map_emotional are available from previous execution
    synthesized_audio_path_emotional_bark_test = synthesize_scene(test_scene_emotional, test_char_map_emotional, backend='bark')

    if synthesized_audio_path_emotional_bark_test and os.path.exists(synthesized_audio_path_emotional_bark_test):
        print(f"Emotional Bark/gTTS synthesis test complete. Output file: {synthesized_audio_path_emotional_bark_test}")
        # Clean up the test output file
        try:
            os.remove(synthesized_audio_path_emotional_bark_test)
            print(f"Cleaned up test file: {synthesized_audio_path_emotional_bark_test}")
        except OSError as e:
            print(f"Error removing test file {synthesized_audio_path_emotional_bark_test}: {e}")
    elif synthesized_audio_path_emotional_bark_test is None:
         print("Emotional Bark/gTTS synthesis test failed or returned None.")
    else:
         print(f"Emotional Bark/gTTS synthesis test complete, but output file {synthesized_audio_path_emotional_bark_test} does not exist.")
except Exception as e:
    print(f"An error occurred during Emotional Bark/gTTS synthesis test: {e}")


# Test synthesize_full with a simple list of scenes
print("\n--- Testing synthesize_full ---")
try:
    # Create a simple list of scenes for the full test
    simple_scenes = [test_scene, test_scene_emotional, test_scene_sfx] # Assuming these test scenes are defined

    # Create a combined char_map for the full test
    combined_char_map = {}
    combined_char_map.update(test_char_map)
    combined_char_map.update(test_char_map_emotional)
    combined_char_map.update(test_char_map_sfx)


    full_audio_path, full_scene_files = synthesize_full(simple_scenes, combined_char_map, backend='gTTS') # Use gTTS for reliability

    if full_audio_path and os.path.exists(full_audio_path):
        print(f"Full story synthesis complete. Output file: {full_audio_path}")
        print(f"Temporary scene files generated: {full_scene_files}") # These should have been cleaned up by synthesize_full

        # Clean up the test output file
        try:
            os.remove(full_audio_path)
            print(f"Cleaned up test file: {full_audio_path}")
        except OSError as e:
            print(f"Error removing test file {full_audio_path}: {e}")

        # Verify that temporary scene files were cleaned up
        print("Verifying temporary scene files were cleaned up...")
        cleaned_up_count = 0
        for sfp in full_scene_files:
            if not os.path.exists(sfp):
                 cleaned_up_count += 1
                 # print(f"Confirmed temporary file cleaned up: {sfp}") # Optional: verbose check
            else:
                 print(f"Temporary file NOT cleaned up: {sfp}")
        print(f"{cleaned_up_count} out of {len(full_scene_files)} temporary scene files were cleaned up.")


    elif full_audio_path is None:
         print("Full story synthesis failed or returned None.")
    else:
         print(f"Full story synthesis complete, but output file {full_audio_path} does not exist.")

except Exception as e:
    print(f"An error occurred during full story synthesis test: {e}")

"""## Summary:

## Summary of the Solving Process

The user's task was to create a Gradio web application capable of taking a story (typed or PDF), analyzing it for characters, dialogue, emotions, and sound effects, and generating an emotional voiceover with multiple voices and background noise.

The solving process involved several subtasks:

1.  **Refine Text Processing:** Enhance character and dialogue detection using spaCy and add basic scene analysis for emotional cues and sound effects. This was successfully implemented, improving the initial text parsing.
2.  **Integrate Advanced TTS Models:** Replace placeholder synthesis with actual TTS models like Bark or Tortoise, aiming for multiple speakers and emotional prosody. Initial attempts to install and integrate Bark faced dependency conflicts and synthesis errors (tensor size mismatch). Tortoise integration was not completed due to installation issues.
3.  **Retry Integrating Advanced TTS Models:** Focus on resolving the Bark tensor error and ensuring a working gTTS fallback. The Bark error persisted, but the gTTS fallback was successfully verified as a functional audio generation method.
4.  **Implement Character-to-Voice Mapping:** Develop a system to map detected characters to specific voice presets. This was implemented using a `char_map` dictionary and integrated into the synthesis functions, although the Bark voice presets still triggered the tensor error, leading to gTTS fallback.
5.  **Enhance Emotional Synthesis:** Utilize TTS capabilities to incorporate emotional cues. The logic to include emotional cues in the Bark prompt was added, but the persistent Bark tensor error prevented testing its effectiveness. The gTTS fallback does not support explicit emotional control.
6.  **Add Background Audio and Sound Effects:** Implement logic to mix background music and sound effects based on analysis. A `SOUND_EFFECT_MAP` was created with placeholder files, and the `synthesize_scene` function was updated to overlay these SFX and mix background music. Temporary file cleanup was also improved.
7.  **Update Gradio Interface:** Modify the UI to include controls for character mapping, voice/emotion selection, and background audio/SFX. The UI was updated using `gr.Dataframe`, `gr.JSON`, and `gr.CheckboxGroup`, and the backend callbacks were wired to handle user input and manage application state.
8.  **Synthesize and Combine Audio:** Ensure the synthesis functions correctly use all gathered information (voices, emotions, SFX, music) and combine the audio into a final output. The functions were refined to incorporate SFX and music mixing and handle temporary file cleanup throughout the pipeline.

## Data Analysis Key Findings

*   The initial text processing successfully integrated spaCy for NER-based character detection and implemented basic analysis for emotional cues and sound effects using keyword matching.
*   Attempts to integrate the Bark TTS model encountered a persistent `ValueError: The size of tensor a (...) must match the size of tensor b (256) at non-singleton dimension 1` during synthesis, preventing its full utilization for multi-voice and emotional synthesis within the current environment.
*   The gTTS fallback mechanism was successfully implemented and verified as a working method for synthesizing speech, although it lacks support for distinct character voices or explicit emotional control.
*   Character-to-voice mapping logic was developed and integrated, allowing assignment of voices (including Bark presets or gTTS default) to characters.
*   The logic for incorporating emotional cues into the Bark prompt was added, but its effect couldn't be evaluated due to the Bark synthesis error.
*   Sound effect mixing based on detected keywords and a predefined map was successfully implemented in the `synthesize_scene` function, allowing overlay of SFX audio.
*   Background music mixing with intensity control was integrated into the synthesis pipeline.
*   The Gradio interface was successfully updated with interactive components for character mapping and SFX selection, and backend callbacks were configured to handle these inputs and manage application state.
*   Temporary audio files generated during synthesis were confirmed to be created and subsequently cleaned up, preventing disk space issues.

## Insights or Next Steps

*   The persistent Bark model tensor size mismatch error needs further investigation and debugging, potentially involving checking model documentation, input tensor shapes, or trying different versions/configurations of the model or `transformers` library.
*   Exploring alternative advanced TTS models (beyond Bark and Tortoise, which had installation issues) that are known to be more stable or easier to integrate in a Colab environment could be beneficial for achieving multi-voice and emotional synthesis if the Bark issue cannot be resolved.

"""